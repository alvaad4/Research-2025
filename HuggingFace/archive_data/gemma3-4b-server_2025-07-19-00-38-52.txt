llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     301.57 ms /    17 tokens (   17.74 ms per token,    56.37 tokens per second)

llama_perf_context_print:        eval time =    5536.21 ms /    55 runs   (  100.66 ms per token,     9.93 tokens per second)

llama_perf_context_print:       total time =    5929.41 ms /    72 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     308.86 ms /    20 tokens (   15.44 ms per token,    64.75 tokens per second)

llama_perf_context_print:        eval time =   17304.74 ms /   172 runs   (  100.61 ms per token,     9.94 tokens per second)

llama_perf_context_print:       total time =   17918.65 ms /   192 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     350.79 ms /    20 tokens (   17.54 ms per token,    57.01 tokens per second)

llama_perf_context_print:        eval time =   30071.20 ms /   298 runs   (  100.91 ms per token,     9.91 tokens per second)

llama_perf_context_print:       total time =   30974.96 ms /   318 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     480.93 ms /    26 tokens (   18.50 ms per token,    54.06 tokens per second)

llama_perf_context_print:        eval time =    6723.20 ms /    67 runs   (  100.35 ms per token,     9.97 tokens per second)

llama_perf_context_print:       total time =    7320.96 ms /    93 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 17, 'completion_tokens': 55, 'total_tokens': 72}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 24, 'completion_tokens': 172, 'total_tokens': 196}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 24, 'completion_tokens': 298, 'total_tokens': 322}}
llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     289.38 ms /    16 tokens (   18.09 ms per token,    55.29 tokens per second)

llama_perf_context_print:        eval time =   11552.21 ms /   115 runs   (  100.45 ms per token,     9.95 tokens per second)

llama_perf_context_print:       total time =   12041.27 ms /   131 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     331.46 ms /    14 tokens (   23.68 ms per token,    42.24 tokens per second)

llama_perf_context_print:        eval time =   14264.24 ms /   142 runs   (  100.45 ms per token,     9.95 tokens per second)

llama_perf_context_print:       total time =   14846.74 ms /   156 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     330.27 ms /    17 tokens (   19.43 ms per token,    51.47 tokens per second)

llama_perf_context_print:        eval time =    2305.74 ms /    23 runs   (  100.25 ms per token,     9.98 tokens per second)

llama_perf_context_print:       total time =    2676.91 ms /    40 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     321.33 ms /    14 tokens (   22.95 ms per token,    43.57 tokens per second)

llama_perf_context_print:        eval time =    2606.15 ms /    26 runs   (  100.24 ms per token,     9.98 tokens per second)

llama_perf_context_print:       total time =    2972.84 ms /    40 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     417.45 ms /    24 tokens (   17.39 ms per token,    57.49 tokens per second)

llama_perf_context_print:        eval time =    1903.30 ms /    19 runs   (  100.17 ms per token,     9.98 tokens per second)

llama_perf_context_print:       total time =    2354.31 ms /    43 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     362.69 ms /    20 tokens (   18.13 ms per token,    55.14 tokens per second)

llama_perf_context_print:        eval time =   10634.20 ms /   106 runs   (  100.32 ms per token,     9.97 tokens per second)

llama_perf_context_print:       total time =   11181.32 ms /   126 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 30, 'completion_tokens': 67, 'total_tokens': 97}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 20, 'completion_tokens': 115, 'total_tokens': 135}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 18, 'completion_tokens': 142, 'total_tokens': 160}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 21, 'completion_tokens': 23, 'total_tokens': 44}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 18, 'completion_tokens': 26, 'total_tokens': 44}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 28, 'completion_tokens': 19, 'total_tokens': 47}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 24, 'completion_tokens': 106, 'total_tokens': 130}}
llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =    1481.33 ms /    93 tokens (   15.93 ms per token,    62.78 tokens per second)

llama_perf_context_print:        eval time =    2716.75 ms /    27 runs   (  100.62 ms per token,     9.94 tokens per second)

llama_perf_context_print:       total time =    4245.24 ms /   120 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     639.78 ms /    38 tokens (   16.84 ms per token,    59.40 tokens per second)

llama_perf_context_print:        eval time =     701.58 ms /     7 runs   (  100.23 ms per token,     9.98 tokens per second)

llama_perf_context_print:       total time =    1354.89 ms /    45 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     815.67 ms /    47 tokens (   17.35 ms per token,    57.62 tokens per second)

llama_perf_context_print:        eval time =     400.39 ms /     4 runs   (  100.10 ms per token,     9.99 tokens per second)

llama_perf_context_print:       total time =    1224.66 ms /    51 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     912.84 ms /    55 tokens (   16.60 ms per token,    60.25 tokens per second)

llama_perf_context_print:        eval time =    4114.79 ms /    41 runs   (  100.36 ms per token,     9.96 tokens per second)

llama_perf_context_print:       total time =    5098.31 ms /    96 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     462.80 ms /    25 tokens (   18.51 ms per token,    54.02 tokens per second)

llama_perf_context_print:        eval time =    3213.50 ms /    32 runs   (  100.42 ms per token,     9.96 tokens per second)

llama_perf_context_print:       total time =    3731.74 ms /    57 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     462.98 ms /    23 tokens (   20.13 ms per token,    49.68 tokens per second)

llama_perf_context_print:        eval time =     199.69 ms /     2 runs   (   99.84 ms per token,    10.02 tokens per second)

llama_perf_context_print:       total time =     667.93 ms /    25 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     820.46 ms /    50 tokens (   16.41 ms per token,    60.94 tokens per second)

llama_perf_context_print:        eval time =    2506.85 ms /    25 runs   (  100.27 ms per token,     9.97 tokens per second)

llama_perf_context_print:       total time =    3370.92 ms /    75 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     899.14 ms /    56 tokens (   16.06 ms per token,    62.28 tokens per second)

llama_perf_context_print:        eval time =    1105.86 ms /    11 runs   (  100.53 ms per token,     9.95 tokens per second)

llama_perf_context_print:       total time =    2025.18 ms /    67 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =    1062.18 ms /    68 tokens (   15.62 ms per token,    64.02 tokens per second)

llama_perf_context_print:        eval time =     802.41 ms /     8 runs   (  100.30 ms per token,     9.97 tokens per second)

llama_perf_context_print:       total time =    1879.82 ms /    76 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =    1602.50 ms /   102 tokens (   15.71 ms per token,    63.65 tokens per second)

llama_perf_context_print:        eval time =    1214.22 ms /    12 runs   (  101.19 ms per token,     9.88 tokens per second)

llama_perf_context_print:       total time =    2838.50 ms /   114 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 97, 'completion_tokens': 27, 'total_tokens': 124}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 42, 'completion_tokens': 7, 'total_tokens': 49}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 51, 'completion_tokens': 4, 'total_tokens': 55}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 59, 'completion_tokens': 41, 'total_tokens': 100}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 29, 'completion_tokens': 32, 'total_tokens': 61}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 27, 'completion_tokens': 2, 'total_tokens': 29}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 54, 'completion_tokens': 25, 'total_tokens': 79}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 63, 'completion_tokens': 11, 'total_tokens': 74}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 75, 'completion_tokens': 8, 'total_tokens': 83}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 106, 'completion_tokens': 12, 'total_tokens': 118}}
llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     857.79 ms /    53 tokens (   16.18 ms per token,    61.79 tokens per second)

llama_perf_context_print:        eval time =   20559.76 ms /   204 runs   (  100.78 ms per token,     9.92 tokens per second)

llama_perf_context_print:       total time =   21780.13 ms /   257 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     872.91 ms /    54 tokens (   16.16 ms per token,    61.86 tokens per second)

llama_perf_context_print:        eval time =   13578.26 ms /   135 runs   (  100.58 ms per token,     9.94 tokens per second)

llama_perf_context_print:       total time =   14687.40 ms /   189 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     589.12 ms /    34 tokens (   17.33 ms per token,    57.71 tokens per second)

llama_perf_context_print:        eval time =   12368.38 ms /   123 runs   (  100.56 ms per token,     9.94 tokens per second)

llama_perf_context_print:       total time =   13172.01 ms /   157 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     803.34 ms /    47 tokens (   17.09 ms per token,    58.51 tokens per second)

llama_perf_context_print:        eval time =   60891.02 ms /   598 runs   (  101.82 ms per token,     9.82 tokens per second)

llama_perf_context_print:       total time =   62886.47 ms /   645 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 57, 'completion_tokens': 204, 'total_tokens': 261}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 58, 'completion_tokens': 135, 'total_tokens': 193}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 38, 'completion_tokens': 123, 'total_tokens': 161}}
llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =    1222.93 ms /    75 tokens (   16.31 ms per token,    61.33 tokens per second)

llama_perf_context_print:        eval time =   37083.42 ms /   366 runs   (  101.32 ms per token,     9.87 tokens per second)

llama_perf_context_print:       total time =   38991.85 ms /   441 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     758.98 ms /    45 tokens (   16.87 ms per token,    59.29 tokens per second)

llama_perf_context_print:        eval time =   11862.22 ms /   118 runs   (  100.53 ms per token,     9.95 tokens per second)

llama_perf_context_print:       total time =   12825.85 ms /   163 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     588.39 ms /    31 tokens (   18.98 ms per token,    52.69 tokens per second)

llama_perf_context_print:        eval time =   12863.43 ms /   128 runs   (  100.50 ms per token,     9.95 tokens per second)

llama_perf_context_print:       total time =   13675.08 ms /   159 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 51, 'completion_tokens': 598, 'total_tokens': 649}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 79, 'completion_tokens': 366, 'total_tokens': 445}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 49, 'completion_tokens': 118, 'total_tokens': 167}}
llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     736.70 ms /    44 tokens (   16.74 ms per token,    59.73 tokens per second)

llama_perf_context_print:        eval time =   12561.42 ms /   125 runs   (  100.49 ms per token,     9.95 tokens per second)

llama_perf_context_print:       total time =   13517.35 ms /   169 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     616.79 ms /    35 tokens (   17.62 ms per token,    56.75 tokens per second)

llama_perf_context_print:        eval time =   16492.73 ms /   164 runs   (  100.57 ms per token,     9.94 tokens per second)

llama_perf_context_print:       total time =   17397.29 ms /   199 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     518.86 ms /    29 tokens (   17.89 ms per token,    55.89 tokens per second)

llama_perf_context_print:        eval time =   39126.83 ms /   387 runs   (  101.10 ms per token,     9.89 tokens per second)

llama_perf_context_print:       total time =   40370.32 ms /   416 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     426.84 ms /    22 tokens (   19.40 ms per token,    51.54 tokens per second)

llama_perf_context_print:        eval time =    2112.17 ms /    21 runs   (  100.58 ms per token,     9.94 tokens per second)

llama_perf_context_print:       total time =    2576.03 ms /    43 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     518.12 ms /    29 tokens (   17.87 ms per token,    55.97 tokens per second)

llama_perf_context_print:        eval time =  102893.77 ms /   999 runs   (  103.00 ms per token,     9.71 tokens per second)

llama_perf_context_print:       total time =  105650.89 ms /  1028 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 35, 'completion_tokens': 128, 'total_tokens': 163}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 48, 'completion_tokens': 125, 'total_tokens': 173}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 39, 'completion_tokens': 164, 'total_tokens': 203}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 33, 'completion_tokens': 387, 'total_tokens': 420}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 27, 'completion_tokens': 21, 'total_tokens': 48}}
llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     618.59 ms /    35 tokens (   17.67 ms per token,    56.58 tokens per second)

llama_perf_context_print:        eval time =   14386.19 ms /   143 runs   (  100.60 ms per token,     9.94 tokens per second)

llama_perf_context_print:       total time =   15255.09 ms /   178 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     381.28 ms /    19 tokens (   20.07 ms per token,    49.83 tokens per second)

llama_perf_context_print:        eval time =   18000.80 ms /   179 runs   (  100.56 ms per token,     9.94 tokens per second)

llama_perf_context_print:       total time =   18701.31 ms /   198 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     362.15 ms /    15 tokens (   24.14 ms per token,    41.42 tokens per second)

llama_perf_context_print:        eval time =   14061.78 ms /   140 runs   (  100.44 ms per token,     9.96 tokens per second)

llama_perf_context_print:       total time =   14668.02 ms /   155 tokens

logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 33, 'completion_tokens': 1000, 'total_tokens': 1033}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 39, 'completion_tokens': 143, 'total_tokens': 182}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 23, 'completion_tokens': 179, 'total_tokens': 202}}
llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     316.28 ms /    17 tokens (   18.60 ms per token,    53.75 tokens per second)

llama_perf_context_print:        eval time =  101865.43 ms /   990 runs   (  102.89 ms per token,     9.72 tokens per second)

llama_perf_context_print:       total time =  104433.21 ms /  1007 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 19, 'completion_tokens': 140, 'total_tokens': 159}}
llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     285.45 ms /    16 tokens (   17.84 ms per token,    56.05 tokens per second)

llama_perf_context_print:        eval time =    3105.08 ms /    31 runs   (  100.16 ms per token,     9.98 tokens per second)

llama_perf_context_print:       total time =    3445.15 ms /    47 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     412.85 ms /    22 tokens (   18.77 ms per token,    53.29 tokens per second)

llama_perf_context_print:        eval time =    3810.53 ms /    38 runs   (  100.28 ms per token,     9.97 tokens per second)

llama_perf_context_print:       total time =    4289.63 ms /    60 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     332.04 ms /    17 tokens (   19.53 ms per token,    51.20 tokens per second)

llama_perf_context_print:        eval time =  102819.21 ms /   999 runs   (  102.92 ms per token,     9.72 tokens per second)

llama_perf_context_print:       total time =  105399.37 ms /  1016 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 21, 'completion_tokens': 990, 'total_tokens': 1011}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 20, 'completion_tokens': 31, 'total_tokens': 51}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 26, 'completion_tokens': 38, 'total_tokens': 64}}
llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     607.23 ms /    37 tokens (   16.41 ms per token,    60.93 tokens per second)

llama_perf_context_print:        eval time =    8533.86 ms /    85 runs   (  100.40 ms per token,     9.96 tokens per second)

llama_perf_context_print:       total time =    9286.53 ms /   122 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =    1415.90 ms /    90 tokens (   15.73 ms per token,    63.56 tokens per second)

llama_perf_context_print:        eval time =    3725.41 ms /    37 runs   (  100.69 ms per token,     9.93 tokens per second)

llama_perf_context_print:       total time =    5204.90 ms /   127 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =    1758.30 ms /   114 tokens (   15.42 ms per token,    64.84 tokens per second)

llama_perf_context_print:        eval time =    7859.44 ms /    78 runs   (  100.76 ms per token,     9.92 tokens per second)

llama_perf_context_print:       total time =    9751.78 ms /   192 tokens

logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 21, 'completion_tokens': 1000, 'total_tokens': 1021}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 41, 'completion_tokens': 85, 'total_tokens': 126}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 94, 'completion_tokens': 37, 'total_tokens': 131}}
llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =    2022.41 ms /   131 tokens (   15.44 ms per token,    64.77 tokens per second)

llama_perf_context_print:        eval time =    4737.24 ms /    47 runs   (  100.79 ms per token,     9.92 tokens per second)

llama_perf_context_print:       total time =    6840.29 ms /   178 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =    1851.46 ms /   119 tokens (   15.56 ms per token,    64.27 tokens per second)

llama_perf_context_print:        eval time =   13837.23 ms /   137 runs   (  101.00 ms per token,     9.90 tokens per second)

llama_perf_context_print:       total time =   15927.99 ms /   256 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =    1201.70 ms /    76 tokens (   15.81 ms per token,    63.24 tokens per second)

llama_perf_context_print:        eval time =    6255.56 ms /    62 runs   (  100.90 ms per token,     9.91 tokens per second)

llama_perf_context_print:       total time =    7565.14 ms /   138 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =    2042.49 ms /   127 tokens (   16.08 ms per token,    62.18 tokens per second)

llama_perf_context_print:        eval time =   16636.70 ms /   164 runs   (  101.44 ms per token,     9.86 tokens per second)

llama_perf_context_print:       total time =   18973.21 ms /   291 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 118, 'completion_tokens': 78, 'total_tokens': 196}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 135, 'completion_tokens': 47, 'total_tokens': 182}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 123, 'completion_tokens': 137, 'total_tokens': 260}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 80, 'completion_tokens': 62, 'total_tokens': 142}}
llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =    1654.32 ms /   104 tokens (   15.91 ms per token,    62.87 tokens per second)

llama_perf_context_print:        eval time =    9614.25 ms /    95 runs   (  101.20 ms per token,     9.88 tokens per second)

llama_perf_context_print:       total time =   11435.75 ms /   199 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     424.28 ms /    24 tokens (   17.68 ms per token,    56.57 tokens per second)

llama_perf_context_print:        eval time =    9063.68 ms /    90 runs   (  100.71 ms per token,     9.93 tokens per second)

llama_perf_context_print:       total time =    9646.07 ms /   114 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =    1070.51 ms /    66 tokens (   16.22 ms per token,    61.65 tokens per second)

llama_perf_context_print:        eval time =    4741.11 ms /    47 runs   (  100.87 ms per token,     9.91 tokens per second)

llama_perf_context_print:       total time =    5893.76 ms /   113 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     409.61 ms /    19 tokens (   21.56 ms per token,    46.39 tokens per second)

llama_perf_context_print:        eval time =    4125.71 ms /    41 runs   (  100.63 ms per token,     9.94 tokens per second)

llama_perf_context_print:       total time =    4606.96 ms /    60 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     696.32 ms /    41 tokens (   16.98 ms per token,    58.88 tokens per second)

llama_perf_context_print:        eval time =    2819.76 ms /    28 runs   (  100.71 ms per token,     9.93 tokens per second)

llama_perf_context_print:       total time =    3565.60 ms /    69 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 131, 'completion_tokens': 164, 'total_tokens': 295}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 108, 'completion_tokens': 95, 'total_tokens': 203}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 28, 'completion_tokens': 90, 'total_tokens': 118}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 70, 'completion_tokens': 47, 'total_tokens': 117}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 23, 'completion_tokens': 41, 'total_tokens': 64}}
llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     999.60 ms /    59 tokens (   16.94 ms per token,    59.02 tokens per second)

llama_perf_context_print:        eval time =    4838.98 ms /    48 runs   (  100.81 ms per token,     9.92 tokens per second)

llama_perf_context_print:       total time =    5922.23 ms /   107 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     930.85 ms /    55 tokens (   16.92 ms per token,    59.09 tokens per second)

llama_perf_context_print:        eval time =    3431.53 ms /    34 runs   (  100.93 ms per token,     9.91 tokens per second)

llama_perf_context_print:       total time =    4421.94 ms /    89 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =    1283.08 ms /    78 tokens (   16.45 ms per token,    60.79 tokens per second)

llama_perf_context_print:        eval time =   12335.93 ms /   122 runs   (  101.11 ms per token,     9.89 tokens per second)

llama_perf_context_print:       total time =   13834.56 ms /   200 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =    1075.52 ms /    66 tokens (   16.30 ms per token,    61.37 tokens per second)

llama_perf_context_print:        eval time =    5042.79 ms /    50 runs   (  100.86 ms per token,     9.92 tokens per second)

llama_perf_context_print:       total time =    6205.32 ms /   116 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     988.30 ms /    60 tokens (   16.47 ms per token,    60.71 tokens per second)

llama_perf_context_print:        eval time =    3424.61 ms /    34 runs   (  100.72 ms per token,     9.93 tokens per second)

llama_perf_context_print:       total time =    4472.44 ms /    94 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     929.40 ms /    55 tokens (   16.90 ms per token,    59.18 tokens per second)

llama_perf_context_print:        eval time =    4836.55 ms /    48 runs   (  100.76 ms per token,     9.92 tokens per second)

llama_perf_context_print:       total time =    5849.59 ms /   103 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 45, 'completion_tokens': 28, 'total_tokens': 73}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 63, 'completion_tokens': 48, 'total_tokens': 111}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 59, 'completion_tokens': 34, 'total_tokens': 93}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 82, 'completion_tokens': 122, 'total_tokens': 204}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 70, 'completion_tokens': 50, 'total_tokens': 120}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 64, 'completion_tokens': 34, 'total_tokens': 98}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 59, 'completion_tokens': 48, 'total_tokens': 107}}
llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     914.08 ms /    56 tokens (   16.32 ms per token,    61.26 tokens per second)

llama_perf_context_print:        eval time =    4738.04 ms /    47 runs   (  100.81 ms per token,     9.92 tokens per second)

llama_perf_context_print:       total time =    5734.15 ms /   103 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     806.71 ms /    46 tokens (   17.54 ms per token,    57.02 tokens per second)

llama_perf_context_print:        eval time =    3222.03 ms /    32 runs   (  100.69 ms per token,     9.93 tokens per second)

llama_perf_context_print:       total time =    4084.73 ms /    78 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =    1083.42 ms /    68 tokens (   15.93 ms per token,    62.76 tokens per second)

llama_perf_context_print:        eval time =    4032.50 ms /    40 runs   (  100.81 ms per token,     9.92 tokens per second)

llama_perf_context_print:       total time =    5185.85 ms /   108 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     326.71 ms /    14 tokens (   23.34 ms per token,    42.85 tokens per second)

llama_perf_context_print:        eval time =    6839.86 ms /    68 runs   (  100.59 ms per token,     9.94 tokens per second)

llama_perf_context_print:       total time =    7285.08 ms /    82 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     998.92 ms /    59 tokens (   16.93 ms per token,    59.06 tokens per second)

llama_perf_context_print:        eval time =    2322.38 ms /    23 runs   (  100.97 ms per token,     9.90 tokens per second)

llama_perf_context_print:       total time =    3362.07 ms /    82 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     277.49 ms /    16 tokens (   17.34 ms per token,    57.66 tokens per second)

llama_perf_context_print:        eval time =    4525.03 ms /    45 runs   (  100.56 ms per token,     9.94 tokens per second)

llama_perf_context_print:       total time =    4881.80 ms /    61 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     815.23 ms /    47 tokens (   17.35 ms per token,    57.65 tokens per second)

llama_perf_context_print:        eval time =    2417.69 ms /    24 runs   (  100.74 ms per token,     9.93 tokens per second)

llama_perf_context_print:       total time =    3275.43 ms /    71 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     353.37 ms /    20 tokens (   17.67 ms per token,    56.60 tokens per second)

llama_perf_context_print:        eval time =   84579.95 ms /   823 runs   (  102.77 ms per token,     9.73 tokens per second)

llama_perf_context_print:       total time =   86730.62 ms /   843 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 60, 'completion_tokens': 47, 'total_tokens': 107}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 50, 'completion_tokens': 32, 'total_tokens': 82}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 72, 'completion_tokens': 40, 'total_tokens': 112}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 18, 'completion_tokens': 68, 'total_tokens': 86}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 63, 'completion_tokens': 23, 'total_tokens': 86}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 20, 'completion_tokens': 45, 'total_tokens': 65}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 51, 'completion_tokens': 24, 'total_tokens': 75}}
llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     784.20 ms /    49 tokens (   16.00 ms per token,    62.48 tokens per second)

llama_perf_context_print:        eval time =    3928.33 ms /    39 runs   (  100.73 ms per token,     9.93 tokens per second)

llama_perf_context_print:       total time =    4780.92 ms /    88 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 24, 'completion_tokens': 823, 'total_tokens': 847}}
llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     297.26 ms /    16 tokens (   18.58 ms per token,    53.82 tokens per second)

llama_perf_context_print:        eval time =    4623.51 ms /    46 runs   (  100.51 ms per token,     9.95 tokens per second)

llama_perf_context_print:       total time =    5001.26 ms /    62 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =    1245.42 ms /    75 tokens (   16.61 ms per token,    60.22 tokens per second)

llama_perf_context_print:        eval time =    1710.82 ms /    17 runs   (  100.64 ms per token,     9.94 tokens per second)

llama_perf_context_print:       total time =    2986.87 ms /    92 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     370.06 ms /    20 tokens (   18.50 ms per token,    54.04 tokens per second)

llama_perf_context_print:        eval time =   42333.53 ms /   417 runs   (  101.52 ms per token,     9.85 tokens per second)

llama_perf_context_print:       total time =   43511.05 ms /   437 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =    1071.93 ms /    63 tokens (   17.01 ms per token,    58.77 tokens per second)

llama_perf_context_print:        eval time =    1309.92 ms /    13 runs   (  100.76 ms per token,     9.92 tokens per second)

llama_perf_context_print:       total time =    2405.71 ms /    76 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     758.93 ms /    43 tokens (   17.65 ms per token,    56.66 tokens per second)

llama_perf_context_print:        eval time =    2416.02 ms /    24 runs   (  100.67 ms per token,     9.93 tokens per second)

llama_perf_context_print:       total time =    3217.34 ms /    67 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 53, 'completion_tokens': 39, 'total_tokens': 92}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 20, 'completion_tokens': 46, 'total_tokens': 66}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 79, 'completion_tokens': 17, 'total_tokens': 96}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 24, 'completion_tokens': 417, 'total_tokens': 441}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 67, 'completion_tokens': 13, 'total_tokens': 80}}
llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     626.16 ms /    35 tokens (   17.89 ms per token,    55.90 tokens per second)

llama_perf_context_print:        eval time =    4232.93 ms /    42 runs   (  100.78 ms per token,     9.92 tokens per second)

llama_perf_context_print:       total time =    4932.56 ms /    77 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     694.61 ms /    39 tokens (   17.81 ms per token,    56.15 tokens per second)

llama_perf_context_print:        eval time =   13011.55 ms /   129 runs   (  100.86 ms per token,     9.91 tokens per second)

llama_perf_context_print:       total time =   13934.51 ms /   168 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     678.10 ms /    40 tokens (   16.95 ms per token,    58.99 tokens per second)

llama_perf_context_print:        eval time =   30501.44 ms /   301 runs   (  101.33 ms per token,     9.87 tokens per second)

llama_perf_context_print:       total time =   31742.98 ms /   341 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     912.77 ms /    56 tokens (   16.30 ms per token,    61.35 tokens per second)

llama_perf_context_print:        eval time =    2518.49 ms /    25 runs   (  100.74 ms per token,     9.93 tokens per second)

llama_perf_context_print:       total time =    3475.35 ms /    81 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 47, 'completion_tokens': 24, 'total_tokens': 71}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 39, 'completion_tokens': 42, 'total_tokens': 81}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 43, 'completion_tokens': 129, 'total_tokens': 172}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 44, 'completion_tokens': 301, 'total_tokens': 345}}
llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     733.59 ms /    44 tokens (   16.67 ms per token,    59.98 tokens per second)

llama_perf_context_print:        eval time =    4931.78 ms /    49 runs   (  100.65 ms per token,     9.94 tokens per second)

llama_perf_context_print:       total time =    5750.89 ms /    93 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     708.49 ms /    41 tokens (   17.28 ms per token,    57.87 tokens per second)

llama_perf_context_print:        eval time =   20222.29 ms /   200 runs   (  101.11 ms per token,     9.89 tokens per second)

llama_perf_context_print:       total time =   21291.88 ms /   241 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     732.87 ms /    44 tokens (   16.66 ms per token,    60.04 tokens per second)

llama_perf_context_print:        eval time =    2115.54 ms /    21 runs   (  100.74 ms per token,     9.93 tokens per second)

llama_perf_context_print:       total time =    2885.87 ms /    65 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     760.98 ms /    45 tokens (   16.91 ms per token,    59.13 tokens per second)

llama_perf_context_print:        eval time =   15450.37 ms /   153 runs   (  100.98 ms per token,     9.90 tokens per second)

llama_perf_context_print:       total time =   16483.86 ms /   198 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     667.29 ms /    38 tokens (   17.56 ms per token,    56.95 tokens per second)

llama_perf_context_print:        eval time =    5237.65 ms /    52 runs   (  100.72 ms per token,     9.93 tokens per second)

llama_perf_context_print:       total time =    5995.73 ms /    90 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 60, 'completion_tokens': 25, 'total_tokens': 85}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 48, 'completion_tokens': 49, 'total_tokens': 97}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 45, 'completion_tokens': 200, 'total_tokens': 245}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 48, 'completion_tokens': 21, 'total_tokens': 69}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 49, 'completion_tokens': 153, 'total_tokens': 202}}
llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =    1018.10 ms /    61 tokens (   16.69 ms per token,    59.92 tokens per second)

llama_perf_context_print:        eval time =   82546.94 ms /   802 runs   (  102.93 ms per token,     9.72 tokens per second)

llama_perf_context_print:       total time =   85326.39 ms /   863 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 42, 'completion_tokens': 52, 'total_tokens': 94}}
llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     766.56 ms /    48 tokens (   15.97 ms per token,    62.62 tokens per second)

llama_perf_context_print:        eval time =   21844.09 ms /   216 runs   (  101.13 ms per token,     9.89 tokens per second)

llama_perf_context_print:       total time =   23006.61 ms /   264 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     927.93 ms /    55 tokens (   16.87 ms per token,    59.27 tokens per second)

llama_perf_context_print:        eval time =   32453.38 ms /   320 runs   (  101.42 ms per token,     9.86 tokens per second)

llama_perf_context_print:       total time =   33987.69 ms /   375 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 65, 'completion_tokens': 802, 'total_tokens': 867}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 52, 'completion_tokens': 216, 'total_tokens': 268}}
llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =    1320.30 ms /    79 tokens (   16.71 ms per token,    59.83 tokens per second)

llama_perf_context_print:        eval time =   85761.09 ms /   832 runs   (  103.08 ms per token,     9.70 tokens per second)

llama_perf_context_print:       total time =   88920.31 ms /   911 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 59, 'completion_tokens': 320, 'total_tokens': 379}}
llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =    1041.38 ms /    62 tokens (   16.80 ms per token,    59.54 tokens per second)

llama_perf_context_print:        eval time =   19414.21 ms /   192 runs   (  101.12 ms per token,     9.89 tokens per second)

llama_perf_context_print:       total time =   20801.52 ms /   254 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =    1080.58 ms /    63 tokens (   17.15 ms per token,    58.30 tokens per second)

llama_perf_context_print:        eval time =   29119.02 ms /   287 runs   (  101.46 ms per token,     9.86 tokens per second)

llama_perf_context_print:       total time =   30732.65 ms /   350 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 83, 'completion_tokens': 832, 'total_tokens': 915}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 66, 'completion_tokens': 192, 'total_tokens': 258}}
llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     985.81 ms /    60 tokens (   16.43 ms per token,    60.86 tokens per second)

llama_perf_context_print:        eval time =  103433.46 ms /   999 runs   (  103.54 ms per token,     9.66 tokens per second)

llama_perf_context_print:       total time =  106717.68 ms /  1059 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 68, 'completion_tokens': 287, 'total_tokens': 355}}
llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     561.63 ms /    33 tokens (   17.02 ms per token,    58.76 tokens per second)

llama_perf_context_print:        eval time =    1912.90 ms /    19 runs   (  100.68 ms per token,     9.93 tokens per second)

llama_perf_context_print:       total time =    2508.55 ms /    52 tokens

llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =     970.02 ms /    58 tokens (   16.72 ms per token,    59.79 tokens per second)

llama_perf_context_print:        eval time =   33911.77 ms /   334 runs   (  101.53 ms per token,     9.85 tokens per second)

llama_perf_context_print:       total time =   35514.00 ms /   392 tokens

logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 64, 'completion_tokens': 1000, 'total_tokens': 1064}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 37, 'completion_tokens': 19, 'total_tokens': 56}}
llama_perf_context_print:        load time =     301.67 ms

llama_perf_context_print: prompt eval time =    1779.37 ms /   111 tokens (   16.03 ms per token,    62.38 tokens per second)

llama_perf_context_print:        eval time =   44873.36 ms /   440 runs   (  101.98 ms per token,     9.81 tokens per second)

llama_perf_context_print:       total time =   47510.63 ms /   551 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 62, 'completion_tokens': 334, 'total_tokens': 396}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 115, 'completion_tokens': 440, 'total_tokens': 555}}
