llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     694.13 ms /    55 tokens (   12.62 ms per token,    79.24 tokens per second)

llama_perf_context_print:        eval time =     449.70 ms /     7 runs   (   64.24 ms per token,    15.57 tokens per second)

llama_perf_context_print:       total time =    1150.85 ms /    62 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     283.72 ms /    21 tokens (   13.51 ms per token,    74.02 tokens per second)

llama_perf_context_print:        eval time =    2835.91 ms /    44 runs   (   64.45 ms per token,    15.52 tokens per second)

llama_perf_context_print:       total time =    3158.02 ms /    65 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     337.50 ms /    21 tokens (   16.07 ms per token,    62.22 tokens per second)

llama_perf_context_print:        eval time =    4203.51 ms /    65 runs   (   64.67 ms per token,    15.46 tokens per second)

llama_perf_context_print:       total time =    4602.94 ms /    86 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     428.97 ms /    28 tokens (   15.32 ms per token,    65.27 tokens per second)

llama_perf_context_print:        eval time =    1160.34 ms /    18 runs   (   64.46 ms per token,    15.51 tokens per second)

llama_perf_context_print:       total time =    1606.45 ms /    46 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     275.85 ms /    14 tokens (   19.70 ms per token,    50.75 tokens per second)

llama_perf_context_print:        eval time =    5810.54 ms /    90 runs   (   64.56 ms per token,    15.49 tokens per second)

llama_perf_context_print:       total time =    6171.40 ms /   104 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     227.00 ms /    16 tokens (   14.19 ms per token,    70.49 tokens per second)

llama_perf_context_print:        eval time =    3610.41 ms /    56 runs   (   64.47 ms per token,    15.51 tokens per second)

llama_perf_context_print:       total time =    3889.55 ms /    72 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     269.08 ms /    17 tokens (   15.83 ms per token,    63.18 tokens per second)

llama_perf_context_print:        eval time =    2190.58 ms /    34 runs   (   64.43 ms per token,    15.52 tokens per second)

llama_perf_context_print:       total time =    2491.23 ms /    51 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     284.66 ms /    15 tokens (   18.98 ms per token,    52.69 tokens per second)

llama_perf_context_print:        eval time =     516.78 ms /     8 runs   (   64.60 ms per token,    15.48 tokens per second)

llama_perf_context_print:       total time =     809.61 ms /    23 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 55, 'completion_tokens': 7, 'total_tokens': 62}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 62, 'completion_tokens': 44, 'total_tokens': 106}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 62, 'completion_tokens': 65, 'total_tokens': 127}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 69, 'completion_tokens': 18, 'total_tokens': 87}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 55, 'completion_tokens': 90, 'total_tokens': 145}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 57, 'completion_tokens': 56, 'total_tokens': 113}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 58, 'completion_tokens': 34, 'total_tokens': 92}}
llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     436.05 ms /    27 tokens (   16.15 ms per token,    61.92 tokens per second)

llama_perf_context_print:        eval time =     902.06 ms /    14 runs   (   64.43 ms per token,    15.52 tokens per second)

llama_perf_context_print:       total time =    1351.59 ms /    41 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     325.25 ms /    21 tokens (   15.49 ms per token,    64.57 tokens per second)

llama_perf_context_print:        eval time =    7827.95 ms /   121 runs   (   64.69 ms per token,    15.46 tokens per second)

llama_perf_context_print:       total time =    8270.05 ms /   142 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =    1300.58 ms /    90 tokens (   14.45 ms per token,    69.20 tokens per second)

llama_perf_context_print:        eval time =   20667.24 ms /   315 runs   (   65.61 ms per token,    15.24 tokens per second)

llama_perf_context_print:       total time =   22312.35 ms /   405 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     571.36 ms /    40 tokens (   14.28 ms per token,    70.01 tokens per second)

llama_perf_context_print:        eval time =   11823.22 ms /   182 runs   (   64.96 ms per token,    15.39 tokens per second)

llama_perf_context_print:       total time =   12577.71 ms /   222 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     746.35 ms /    51 tokens (   14.63 ms per token,    68.33 tokens per second)

llama_perf_context_print:        eval time =    9415.03 ms /   145 runs   (   64.93 ms per token,    15.40 tokens per second)

llama_perf_context_print:       total time =   10303.34 ms /   196 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 56, 'completion_tokens': 8, 'total_tokens': 64}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 68, 'completion_tokens': 14, 'total_tokens': 82}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 62, 'completion_tokens': 121, 'total_tokens': 183}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 131, 'completion_tokens': 315, 'total_tokens': 446}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 81, 'completion_tokens': 182, 'total_tokens': 263}}
llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     839.25 ms /    58 tokens (   14.47 ms per token,    69.11 tokens per second)

llama_perf_context_print:        eval time =    3948.04 ms /    61 runs   (   64.72 ms per token,    15.45 tokens per second)

llama_perf_context_print:       total time =    4844.01 ms /   119 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     463.98 ms /    29 tokens (   16.00 ms per token,    62.50 tokens per second)

llama_perf_context_print:        eval time =    5109.34 ms /    79 runs   (   64.68 ms per token,    15.46 tokens per second)

llama_perf_context_print:       total time =    5647.53 ms /   108 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     362.20 ms /    24 tokens (   15.09 ms per token,    66.26 tokens per second)

llama_perf_context_print:        eval time =   18107.67 ms /   278 runs   (   65.14 ms per token,    15.35 tokens per second)

llama_perf_context_print:       total time =   18765.05 ms /   302 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     747.53 ms /    51 tokens (   14.66 ms per token,    68.23 tokens per second)

llama_perf_context_print:        eval time =    3947.26 ms /    61 runs   (   64.71 ms per token,    15.45 tokens per second)

llama_perf_context_print:       total time =    4751.34 ms /   112 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 92, 'completion_tokens': 145, 'total_tokens': 237}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 99, 'completion_tokens': 61, 'total_tokens': 160}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 69, 'completion_tokens': 79, 'total_tokens': 148}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 64, 'completion_tokens': 278, 'total_tokens': 342}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 92, 'completion_tokens': 61, 'total_tokens': 153}}
llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     840.50 ms /    58 tokens (   14.49 ms per token,    69.01 tokens per second)

llama_perf_context_print:        eval time =   20142.27 ms /   308 runs   (   65.40 ms per token,    15.29 tokens per second)

llama_perf_context_print:       total time =   21315.85 ms /   366 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     948.06 ms /    63 tokens (   15.05 ms per token,    66.45 tokens per second)

llama_perf_context_print:        eval time =   14871.06 ms /   228 runs   (   65.22 ms per token,    15.33 tokens per second)

llama_perf_context_print:       total time =   16055.50 ms /   291 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =    1437.31 ms /   102 tokens (   14.09 ms per token,    70.97 tokens per second)

llama_perf_context_print:        eval time =   11099.53 ms /   170 runs   (   65.29 ms per token,    15.32 tokens per second)

llama_perf_context_print:       total time =   12706.54 ms /   272 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 102, 'completion_tokens': 308, 'total_tokens': 410}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 107, 'completion_tokens': 228, 'total_tokens': 335}}
llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     738.72 ms /    51 tokens (   14.48 ms per token,    69.04 tokens per second)

llama_perf_context_print:        eval time =   16574.11 ms /   254 runs   (   65.25 ms per token,    15.33 tokens per second)

llama_perf_context_print:       total time =   17579.17 ms /   305 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     783.23 ms /    54 tokens (   14.50 ms per token,    68.95 tokens per second)

llama_perf_context_print:        eval time =   10269.02 ms /   158 runs   (   64.99 ms per token,    15.39 tokens per second)

llama_perf_context_print:       total time =   11207.53 ms /   212 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     503.62 ms /    34 tokens (   14.81 ms per token,    67.51 tokens per second)

llama_perf_context_print:        eval time =    8355.25 ms /   129 runs   (   64.77 ms per token,    15.44 tokens per second)

llama_perf_context_print:       total time =    8983.75 ms /   163 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     673.68 ms /    45 tokens (   14.97 ms per token,    66.80 tokens per second)

llama_perf_context_print:        eval time =   19870.07 ms /   304 runs   (   65.36 ms per token,    15.30 tokens per second)

llama_perf_context_print:       total time =   20870.12 ms /   349 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 143, 'completion_tokens': 170, 'total_tokens': 313}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 91, 'completion_tokens': 254, 'total_tokens': 345}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 94, 'completion_tokens': 158, 'total_tokens': 252}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 75, 'completion_tokens': 129, 'total_tokens': 204}}
llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =    1085.46 ms /    75 tokens (   14.47 ms per token,    69.10 tokens per second)

llama_perf_context_print:        eval time =   27037.38 ms /   411 runs   (   65.78 ms per token,    15.20 tokens per second)

llama_perf_context_print:       total time =   28595.04 ms /   486 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     635.60 ms /    42 tokens (   15.13 ms per token,    66.08 tokens per second)

llama_perf_context_print:        eval time =    8822.79 ms /   136 runs   (   64.87 ms per token,    15.41 tokens per second)

llama_perf_context_print:       total time =    9590.53 ms /   178 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 86, 'completion_tokens': 304, 'total_tokens': 390}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 116, 'completion_tokens': 411, 'total_tokens': 527}}
llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     448.89 ms /    29 tokens (   15.48 ms per token,    64.60 tokens per second)

llama_perf_context_print:        eval time =   18908.85 ms /   290 runs   (   65.20 ms per token,    15.34 tokens per second)

llama_perf_context_print:       total time =   19667.59 ms /   319 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     656.90 ms /    43 tokens (   15.28 ms per token,    65.46 tokens per second)

llama_perf_context_print:        eval time =    7989.15 ms /   123 runs   (   64.95 ms per token,    15.40 tokens per second)

llama_perf_context_print:       total time =    8764.36 ms /   166 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     542.27 ms /    35 tokens (   15.49 ms per token,    64.54 tokens per second)

llama_perf_context_print:        eval time =    7491.10 ms /   114 runs   (   65.71 ms per token,    15.22 tokens per second)

llama_perf_context_print:       total time =    8149.12 ms /   149 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     659.70 ms /    28 tokens (   23.56 ms per token,    42.44 tokens per second)

llama_perf_context_print:        eval time =    9490.44 ms /   144 runs   (   65.91 ms per token,    15.17 tokens per second)

llama_perf_context_print:       total time =   10301.00 ms /   172 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =    1058.36 ms /    21 tokens (   50.40 ms per token,    19.84 tokens per second)

llama_perf_context_print:        eval time =    3065.57 ms /    45 runs   (   68.12 ms per token,    14.68 tokens per second)

llama_perf_context_print:       total time =    4167.35 ms /    66 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 83, 'completion_tokens': 136, 'total_tokens': 219}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 70, 'completion_tokens': 290, 'total_tokens': 360}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 84, 'completion_tokens': 123, 'total_tokens': 207}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 76, 'completion_tokens': 114, 'total_tokens': 190}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 69, 'completion_tokens': 144, 'total_tokens': 213}}
llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     400.45 ms /    26 tokens (   15.40 ms per token,    64.93 tokens per second)

llama_perf_context_print:        eval time =   25620.48 ms /   393 runs   (   65.19 ms per token,    15.34 tokens per second)

llama_perf_context_print:       total time =   26457.10 ms /   419 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     479.90 ms /    33 tokens (   14.54 ms per token,    68.76 tokens per second)

llama_perf_context_print:        eval time =    4896.88 ms /    76 runs   (   64.43 ms per token,    15.52 tokens per second)

llama_perf_context_print:       total time =    5447.37 ms /   109 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     294.47 ms /    20 tokens (   14.72 ms per token,    67.92 tokens per second)

llama_perf_context_print:        eval time =   11955.07 ms /   185 runs   (   64.62 ms per token,    15.47 tokens per second)

llama_perf_context_print:       total time =   12432.29 ms /   205 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     283.72 ms /    15 tokens (   18.91 ms per token,    52.87 tokens per second)

llama_perf_context_print:        eval time =    1732.89 ms /    27 runs   (   64.18 ms per token,    15.58 tokens per second)

llama_perf_context_print:       total time =    2041.17 ms /    42 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 63, 'completion_tokens': 45, 'total_tokens': 108}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 66, 'completion_tokens': 393, 'total_tokens': 459}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 73, 'completion_tokens': 76, 'total_tokens': 149}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 61, 'completion_tokens': 185, 'total_tokens': 246}}
llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     314.94 ms /    19 tokens (   16.58 ms per token,    60.33 tokens per second)

llama_perf_context_print:        eval time =   26428.63 ms /   405 runs   (   65.26 ms per token,    15.32 tokens per second)

llama_perf_context_print:       total time =   27204.94 ms /   424 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     262.68 ms /    14 tokens (   18.76 ms per token,    53.30 tokens per second)

llama_perf_context_print:        eval time =     644.00 ms /    10 runs   (   64.40 ms per token,    15.53 tokens per second)

llama_perf_context_print:       total time =     916.58 ms /    24 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     378.77 ms /    23 tokens (   16.47 ms per token,    60.72 tokens per second)

llama_perf_context_print:        eval time =    1226.75 ms /    19 runs   (   64.57 ms per token,    15.49 tokens per second)

llama_perf_context_print:       total time =    1623.41 ms /    42 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     289.69 ms /    18 tokens (   16.09 ms per token,    62.14 tokens per second)

llama_perf_context_print:        eval time =   30048.71 ms /   458 runs   (   65.61 ms per token,    15.24 tokens per second)

llama_perf_context_print:       total time =   30886.72 ms /   476 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 56, 'completion_tokens': 27, 'total_tokens': 83}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 60, 'completion_tokens': 405, 'total_tokens': 465}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 55, 'completion_tokens': 10, 'total_tokens': 65}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 64, 'completion_tokens': 19, 'total_tokens': 83}}
llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     524.47 ms /    35 tokens (   14.98 ms per token,    66.73 tokens per second)

llama_perf_context_print:        eval time =    5758.15 ms /    89 runs   (   64.70 ms per token,    15.46 tokens per second)

llama_perf_context_print:       total time =    6367.11 ms /   124 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =    1330.83 ms /    93 tokens (   14.31 ms per token,    69.88 tokens per second)

llama_perf_context_print:        eval time =    3250.49 ms /    50 runs   (   65.01 ms per token,    15.38 tokens per second)

llama_perf_context_print:       total time =    4628.35 ms /   143 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =    1587.04 ms /   111 tokens (   14.30 ms per token,    69.94 tokens per second)

llama_perf_context_print:        eval time =    5277.43 ms /    81 runs   (   65.15 ms per token,    15.35 tokens per second)

llama_perf_context_print:       total time =    6941.40 ms /   192 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 59, 'completion_tokens': 458, 'total_tokens': 517}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 76, 'completion_tokens': 89, 'total_tokens': 165}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 134, 'completion_tokens': 50, 'total_tokens': 184}}
llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =    1847.04 ms /   133 tokens (   13.89 ms per token,    72.01 tokens per second)

llama_perf_context_print:        eval time =    4106.16 ms /    63 runs   (   65.18 ms per token,    15.34 tokens per second)

llama_perf_context_print:       total time =    6012.45 ms /   196 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =    1624.02 ms /   117 tokens (   13.88 ms per token,    72.04 tokens per second)

llama_perf_context_print:        eval time =    3969.45 ms /    61 runs   (   65.07 ms per token,    15.37 tokens per second)

llama_perf_context_print:       total time =    5650.72 ms /   178 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =    1085.30 ms /    76 tokens (   14.28 ms per token,    70.03 tokens per second)

llama_perf_context_print:        eval time =    1035.20 ms /    16 runs   (   64.70 ms per token,    15.46 tokens per second)

llama_perf_context_print:       total time =    2135.79 ms /    92 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =    1856.47 ms /   133 tokens (   13.96 ms per token,    71.64 tokens per second)

llama_perf_context_print:        eval time =    5478.56 ms /    84 runs   (   65.22 ms per token,    15.33 tokens per second)

llama_perf_context_print:       total time =    7414.76 ms /   217 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 152, 'completion_tokens': 81, 'total_tokens': 233}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 174, 'completion_tokens': 63, 'total_tokens': 237}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 158, 'completion_tokens': 61, 'total_tokens': 219}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 117, 'completion_tokens': 16, 'total_tokens': 133}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 174, 'completion_tokens': 84, 'total_tokens': 258}}
llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =    1373.72 ms /    98 tokens (   14.02 ms per token,    71.34 tokens per second)

llama_perf_context_print:        eval time =    5862.52 ms /    90 runs   (   65.14 ms per token,    15.35 tokens per second)

llama_perf_context_print:       total time =    7321.74 ms /   188 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     455.27 ms /    27 tokens (   16.86 ms per token,    59.30 tokens per second)

llama_perf_context_print:        eval time =    7437.60 ms /   115 runs   (   64.67 ms per token,    15.46 tokens per second)

llama_perf_context_print:       total time =    8003.61 ms /   142 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     965.56 ms /    67 tokens (   14.41 ms per token,    69.39 tokens per second)

llama_perf_context_print:        eval time =    2071.77 ms /    32 runs   (   64.74 ms per token,    15.45 tokens per second)

llama_perf_context_print:       total time =    3067.27 ms /    99 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     365.60 ms /    22 tokens (   16.62 ms per token,    60.17 tokens per second)

llama_perf_context_print:        eval time =    3548.66 ms /    55 runs   (   64.52 ms per token,    15.50 tokens per second)

llama_perf_context_print:       total time =    3965.55 ms /    77 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     660.17 ms /    43 tokens (   15.35 ms per token,    65.13 tokens per second)

llama_perf_context_print:        eval time =    2066.60 ms /    32 runs   (   64.58 ms per token,    15.48 tokens per second)

llama_perf_context_print:       total time =    2756.56 ms /    75 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     879.16 ms /    59 tokens (   14.90 ms per token,    67.11 tokens per second)

llama_perf_context_print:        eval time =    3310.48 ms /    51 runs   (   64.91 ms per token,    15.41 tokens per second)

llama_perf_context_print:       total time =    4237.09 ms /   110 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     764.40 ms /    53 tokens (   14.42 ms per token,    69.34 tokens per second)

llama_perf_context_print:        eval time =    2005.43 ms /    31 runs   (   64.69 ms per token,    15.46 tokens per second)

llama_perf_context_print:       total time =    2798.69 ms /    84 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 139, 'completion_tokens': 90, 'total_tokens': 229}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 68, 'completion_tokens': 115, 'total_tokens': 183}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 108, 'completion_tokens': 32, 'total_tokens': 140}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 63, 'completion_tokens': 55, 'total_tokens': 118}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 84, 'completion_tokens': 32, 'total_tokens': 116}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 100, 'completion_tokens': 51, 'total_tokens': 151}}
llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =    1089.54 ms /    76 tokens (   14.34 ms per token,    69.75 tokens per second)

llama_perf_context_print:        eval time =    5647.44 ms /    87 runs   (   64.91 ms per token,    15.41 tokens per second)

llama_perf_context_print:       total time =    6819.40 ms /   163 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     943.41 ms /    66 tokens (   14.29 ms per token,    69.96 tokens per second)

llama_perf_context_print:        eval time =    2718.26 ms /    42 runs   (   64.72 ms per token,    15.45 tokens per second)

llama_perf_context_print:       total time =    3700.80 ms /   108 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     893.10 ms /    61 tokens (   14.64 ms per token,    68.30 tokens per second)

llama_perf_context_print:        eval time =    3173.43 ms /    49 runs   (   64.76 ms per token,    15.44 tokens per second)

llama_perf_context_print:       total time =    4112.17 ms /   110 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     725.15 ms /    52 tokens (   13.95 ms per token,    71.71 tokens per second)

llama_perf_context_print:        eval time =    2716.79 ms /    42 runs   (   64.69 ms per token,    15.46 tokens per second)

llama_perf_context_print:       total time =    3481.19 ms /    94 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     818.57 ms /    55 tokens (   14.88 ms per token,    67.19 tokens per second)

llama_perf_context_print:        eval time =    2586.91 ms /    40 runs   (   64.67 ms per token,    15.46 tokens per second)

llama_perf_context_print:       total time =    3442.63 ms /    95 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     674.19 ms /    45 tokens (   14.98 ms per token,    66.75 tokens per second)

llama_perf_context_print:        eval time =    1874.99 ms /    29 runs   (   64.65 ms per token,    15.47 tokens per second)

llama_perf_context_print:       total time =    2576.18 ms /    74 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     899.24 ms /    61 tokens (   14.74 ms per token,    67.84 tokens per second)

llama_perf_context_print:        eval time =    2592.93 ms /    40 runs   (   64.82 ms per token,    15.43 tokens per second)

llama_perf_context_print:       total time =    3529.51 ms /   101 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 94, 'completion_tokens': 31, 'total_tokens': 125}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 117, 'completion_tokens': 87, 'total_tokens': 204}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 107, 'completion_tokens': 42, 'total_tokens': 149}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 102, 'completion_tokens': 49, 'total_tokens': 151}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 93, 'completion_tokens': 42, 'total_tokens': 135}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 96, 'completion_tokens': 40, 'total_tokens': 136}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 86, 'completion_tokens': 29, 'total_tokens': 115}}
llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     283.33 ms /    15 tokens (   18.89 ms per token,    52.94 tokens per second)

llama_perf_context_print:        eval time =    2771.08 ms /    43 runs   (   64.44 ms per token,    15.52 tokens per second)

llama_perf_context_print:       total time =    3094.31 ms /    58 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     877.07 ms /    59 tokens (   14.87 ms per token,    67.27 tokens per second)

llama_perf_context_print:        eval time =    1292.78 ms /    20 runs   (   64.64 ms per token,    15.47 tokens per second)

llama_perf_context_print:       total time =    2188.71 ms /    79 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     263.36 ms /    17 tokens (   15.49 ms per token,    64.55 tokens per second)

llama_perf_context_print:        eval time =     453.15 ms /     7 runs   (   64.74 ms per token,    15.45 tokens per second)

llama_perf_context_print:       total time =     723.77 ms /    24 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     707.43 ms /    47 tokens (   15.05 ms per token,    66.44 tokens per second)

llama_perf_context_print:        eval time =    1486.35 ms /    23 runs   (   64.62 ms per token,    15.47 tokens per second)

llama_perf_context_print:       total time =    2215.38 ms /    70 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     321.84 ms /    21 tokens (   15.33 ms per token,    65.25 tokens per second)

llama_perf_context_print:        eval time =    1031.31 ms /    16 runs   (   64.46 ms per token,    15.51 tokens per second)

llama_perf_context_print:       total time =    1368.41 ms /    37 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     699.40 ms /    49 tokens (   14.27 ms per token,    70.06 tokens per second)

llama_perf_context_print:        eval time =    1162.90 ms /    18 runs   (   64.61 ms per token,    15.48 tokens per second)

llama_perf_context_print:       total time =    1879.34 ms /    67 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     263.69 ms /    17 tokens (   15.51 ms per token,    64.47 tokens per second)

llama_perf_context_print:        eval time =     773.36 ms /    12 runs   (   64.45 ms per token,    15.52 tokens per second)

llama_perf_context_print:       total time =    1048.74 ms /    29 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =    1026.18 ms /    71 tokens (   14.45 ms per token,    69.19 tokens per second)

llama_perf_context_print:        eval time =    1230.78 ms /    19 runs   (   64.78 ms per token,    15.44 tokens per second)

llama_perf_context_print:       total time =    2274.92 ms /    90 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     310.59 ms /    20 tokens (   15.53 ms per token,    64.39 tokens per second)

llama_perf_context_print:        eval time =    1159.65 ms /    18 runs   (   64.42 ms per token,    15.52 tokens per second)

llama_perf_context_print:       total time =    1487.21 ms /    38 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 102, 'completion_tokens': 40, 'total_tokens': 142}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 56, 'completion_tokens': 43, 'total_tokens': 99}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 100, 'completion_tokens': 20, 'total_tokens': 120}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 58, 'completion_tokens': 7, 'total_tokens': 65}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 88, 'completion_tokens': 23, 'total_tokens': 111}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 62, 'completion_tokens': 16, 'total_tokens': 78}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 90, 'completion_tokens': 18, 'total_tokens': 108}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 58, 'completion_tokens': 12, 'total_tokens': 70}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 112, 'completion_tokens': 19, 'total_tokens': 131}}
llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     895.31 ms /    61 tokens (   14.68 ms per token,    68.13 tokens per second)

llama_perf_context_print:        eval time =    1164.07 ms /    18 runs   (   64.67 ms per token,    15.46 tokens per second)

llama_perf_context_print:       total time =    2076.45 ms /    79 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     656.37 ms /    44 tokens (   14.92 ms per token,    67.04 tokens per second)

llama_perf_context_print:        eval time =    1678.19 ms /    26 runs   (   64.55 ms per token,    15.49 tokens per second)

llama_perf_context_print:       total time =    2358.91 ms /    70 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     549.80 ms /    37 tokens (   14.86 ms per token,    67.30 tokens per second)

llama_perf_context_print:        eval time =    3041.16 ms /    47 runs   (   64.71 ms per token,    15.45 tokens per second)

llama_perf_context_print:       total time =    3634.93 ms /    84 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     590.53 ms /    40 tokens (   14.76 ms per token,    67.74 tokens per second)

llama_perf_context_print:        eval time =    3170.30 ms /    49 runs   (   64.70 ms per token,    15.46 tokens per second)

llama_perf_context_print:       total time =    3806.76 ms /    89 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     642.60 ms /    43 tokens (   14.94 ms per token,    66.92 tokens per second)

llama_perf_context_print:        eval time =    1486.50 ms /    23 runs   (   64.63 ms per token,    15.47 tokens per second)

llama_perf_context_print:       total time =    2150.63 ms /    66 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     808.69 ms /    55 tokens (   14.70 ms per token,    68.01 tokens per second)

llama_perf_context_print:        eval time =    2716.53 ms /    42 runs   (   64.68 ms per token,    15.46 tokens per second)

llama_perf_context_print:       total time =    3564.70 ms /    97 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     647.56 ms /    43 tokens (   15.06 ms per token,    66.40 tokens per second)

llama_perf_context_print:        eval time =     774.38 ms /    12 runs   (   64.53 ms per token,    15.50 tokens per second)

llama_perf_context_print:       total time =    1433.70 ms /    55 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     658.77 ms /    43 tokens (   15.32 ms per token,    65.27 tokens per second)

llama_perf_context_print:        eval time =    3297.07 ms /    51 runs   (   64.65 ms per token,    15.47 tokens per second)

llama_perf_context_print:       total time =    4003.34 ms /    94 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 60, 'completion_tokens': 18, 'total_tokens': 78}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 101, 'completion_tokens': 18, 'total_tokens': 119}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 85, 'completion_tokens': 26, 'total_tokens': 111}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 78, 'completion_tokens': 47, 'total_tokens': 125}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 81, 'completion_tokens': 49, 'total_tokens': 130}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 84, 'completion_tokens': 23, 'total_tokens': 107}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 96, 'completion_tokens': 42, 'total_tokens': 138}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 84, 'completion_tokens': 12, 'total_tokens': 96}}
llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     676.98 ms /    45 tokens (   15.04 ms per token,    66.47 tokens per second)

llama_perf_context_print:        eval time =    2007.04 ms /    31 runs   (   64.74 ms per token,    15.45 tokens per second)

llama_perf_context_print:       total time =    2712.97 ms /    76 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     686.75 ms /    46 tokens (   14.93 ms per token,    66.98 tokens per second)

llama_perf_context_print:        eval time =    2977.75 ms /    46 runs   (   64.73 ms per token,    15.45 tokens per second)

llama_perf_context_print:       total time =    3707.37 ms /    92 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     549.77 ms /    37 tokens (   14.86 ms per token,    67.30 tokens per second)

llama_perf_context_print:        eval time =    5761.40 ms /    89 runs   (   64.73 ms per token,    15.45 tokens per second)

llama_perf_context_print:       total time =    6395.64 ms /   126 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     837.63 ms /    57 tokens (   14.70 ms per token,    68.05 tokens per second)

llama_perf_context_print:        eval time =   15308.81 ms /   235 runs   (   65.14 ms per token,    15.35 tokens per second)

llama_perf_context_print:       total time =   16390.52 ms /   292 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     654.49 ms /    44 tokens (   14.87 ms per token,    67.23 tokens per second)

llama_perf_context_print:        eval time =   13712.31 ms /   211 runs   (   64.99 ms per token,    15.39 tokens per second)

llama_perf_context_print:       total time =   14582.44 ms /   255 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 84, 'completion_tokens': 51, 'total_tokens': 135}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 86, 'completion_tokens': 31, 'total_tokens': 117}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 87, 'completion_tokens': 46, 'total_tokens': 133}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 78, 'completion_tokens': 89, 'total_tokens': 167}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 87, 'completion_tokens': 235, 'total_tokens': 322}}
llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     733.13 ms /    51 tokens (   14.38 ms per token,    69.56 tokens per second)

llama_perf_context_print:        eval time =   12226.49 ms /   188 runs   (   65.03 ms per token,    15.38 tokens per second)

llama_perf_context_print:       total time =   13149.71 ms /   239 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =    1086.92 ms /    76 tokens (   14.30 ms per token,    69.92 tokens per second)

llama_perf_context_print:        eval time =   24420.13 ms /   372 runs   (   65.65 ms per token,    15.23 tokens per second)

llama_perf_context_print:       total time =   25928.20 ms /   448 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 74, 'completion_tokens': 211, 'total_tokens': 285}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 81, 'completion_tokens': 188, 'total_tokens': 269}}
llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     801.89 ms /    56 tokens (   14.32 ms per token,    69.84 tokens per second)

llama_perf_context_print:        eval time =    9355.42 ms /   144 runs   (   64.97 ms per token,    15.39 tokens per second)

llama_perf_context_print:       total time =   10298.45 ms /   200 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     847.52 ms /    58 tokens (   14.61 ms per token,    68.43 tokens per second)

llama_perf_context_print:        eval time =   17356.24 ms /   266 runs   (   65.25 ms per token,    15.33 tokens per second)

llama_perf_context_print:       total time =   18484.56 ms /   324 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     784.48 ms /    56 tokens (   14.01 ms per token,    71.39 tokens per second)

llama_perf_context_print:        eval time =   30845.53 ms /   469 runs   (   65.77 ms per token,    15.20 tokens per second)

llama_perf_context_print:       total time =   32189.25 ms /   525 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 106, 'completion_tokens': 372, 'total_tokens': 478}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 86, 'completion_tokens': 144, 'total_tokens': 230}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 89, 'completion_tokens': 266, 'total_tokens': 355}}
llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     447.62 ms /    29 tokens (   15.44 ms per token,    64.79 tokens per second)

llama_perf_context_print:        eval time =     388.89 ms /     6 runs   (   64.82 ms per token,    15.43 tokens per second)

llama_perf_context_print:       total time =     842.95 ms /    35 tokens

llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =     793.17 ms /    55 tokens (   14.42 ms per token,    69.34 tokens per second)

llama_perf_context_print:        eval time =   21645.64 ms /   331 runs   (   65.39 ms per token,    15.29 tokens per second)

llama_perf_context_print:       total time =   22802.82 ms /   386 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 86, 'completion_tokens': 469, 'total_tokens': 555}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 59, 'completion_tokens': 6, 'total_tokens': 65}}
llama_perf_context_print:        load time =     694.31 ms

llama_perf_context_print: prompt eval time =    1548.34 ms /   107 tokens (   14.47 ms per token,    69.11 tokens per second)

llama_perf_context_print:        eval time =   14520.28 ms /   222 runs   (   65.41 ms per token,    15.29 tokens per second)

llama_perf_context_print:       total time =   16301.30 ms /   329 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 85, 'completion_tokens': 331, 'total_tokens': 416}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 137, 'completion_tokens': 222, 'total_tokens': 359}}
