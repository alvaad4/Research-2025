llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     239.72 ms /    35 tokens (    6.85 ms per token,   146.01 tokens per second)

llama_perf_context_print:        eval time =     120.05 ms /     7 runs   (   17.15 ms per token,    58.31 tokens per second)

llama_perf_context_print:       total time =     375.20 ms /    42 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     112.18 ms /    19 tokens (    5.90 ms per token,   169.36 tokens per second)

llama_perf_context_print:        eval time =     120.52 ms /     7 runs   (   17.22 ms per token,    58.08 tokens per second)

llama_perf_context_print:       total time =     246.65 ms /    26 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     106.87 ms /    19 tokens (    5.62 ms per token,   177.78 tokens per second)

llama_perf_context_print:        eval time =     498.70 ms /    31 runs   (   16.09 ms per token,    62.16 tokens per second)

llama_perf_context_print:       total time =     658.84 ms /    50 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     121.87 ms /    26 tokens (    4.69 ms per token,   213.34 tokens per second)

llama_perf_context_print:        eval time =     325.10 ms /    20 runs   (   16.25 ms per token,    61.52 tokens per second)

llama_perf_context_print:       total time =     484.55 ms /    46 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     104.31 ms /    13 tokens (    8.02 ms per token,   124.62 tokens per second)

llama_perf_context_print:        eval time =     180.61 ms /    11 runs   (   16.42 ms per token,    60.91 tokens per second)

llama_perf_context_print:       total time =     304.98 ms /    24 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     114.94 ms /    14 tokens (    8.21 ms per token,   121.80 tokens per second)

llama_perf_context_print:        eval time =     471.03 ms /    29 runs   (   16.24 ms per token,    61.57 tokens per second)

llama_perf_context_print:       total time =     635.49 ms /    43 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     100.84 ms /    16 tokens (    6.30 ms per token,   158.67 tokens per second)

llama_perf_context_print:        eval time =     491.71 ms /    30 runs   (   16.39 ms per token,    61.01 tokens per second)

llama_perf_context_print:       total time =     644.58 ms /    46 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     113.54 ms /    13 tokens (    8.73 ms per token,   114.50 tokens per second)

llama_perf_context_print:        eval time =     152.04 ms /     9 runs   (   16.89 ms per token,    59.20 tokens per second)

llama_perf_context_print:       total time =     282.70 ms /    22 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     121.68 ms /    25 tokens (    4.87 ms per token,   205.45 tokens per second)

llama_perf_context_print:        eval time =    1291.21 ms /    78 runs   (   16.55 ms per token,    60.41 tokens per second)

llama_perf_context_print:       total time =    1546.94 ms /   103 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 35, 'completion_tokens': 7, 'total_tokens': 42}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 42, 'completion_tokens': 7, 'total_tokens': 49}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 42, 'completion_tokens': 31, 'total_tokens': 73}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 49, 'completion_tokens': 20, 'total_tokens': 69}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 36, 'completion_tokens': 11, 'total_tokens': 47}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 37, 'completion_tokens': 29, 'total_tokens': 66}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 39, 'completion_tokens': 30, 'total_tokens': 69}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 36, 'completion_tokens': 9, 'total_tokens': 45}}
llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     106.16 ms /    20 tokens (    5.31 ms per token,   188.39 tokens per second)

llama_perf_context_print:        eval time =    2298.89 ms /   136 runs   (   16.90 ms per token,    59.16 tokens per second)

llama_perf_context_print:       total time =    2645.67 ms /   156 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     258.05 ms /    89 tokens (    2.90 ms per token,   344.89 tokens per second)

llama_perf_context_print:        eval time =     306.83 ms /    17 runs   (   18.05 ms per token,    55.41 tokens per second)

llama_perf_context_print:       total time =     596.05 ms /   106 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     142.66 ms /    39 tokens (    3.66 ms per token,   273.37 tokens per second)

llama_perf_context_print:        eval time =     531.86 ms /    32 runs   (   16.62 ms per token,    60.17 tokens per second)

llama_perf_context_print:       total time =     729.01 ms /    71 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     165.53 ms /    50 tokens (    3.31 ms per token,   302.05 tokens per second)

llama_perf_context_print:        eval time =     561.86 ms /    34 runs   (   16.53 ms per token,    60.51 tokens per second)

llama_perf_context_print:       total time =     785.14 ms /    84 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     201.21 ms /    57 tokens (    3.53 ms per token,   283.29 tokens per second)

llama_perf_context_print:        eval time =     775.70 ms /    47 runs   (   16.50 ms per token,    60.59 tokens per second)

llama_perf_context_print:       total time =    1053.38 ms /   104 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     134.94 ms /    26 tokens (    5.19 ms per token,   192.68 tokens per second)

llama_perf_context_print:        eval time =     301.78 ms /    18 runs   (   16.77 ms per token,    59.65 tokens per second)

llama_perf_context_print:       total time =     468.96 ms /    44 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     115.60 ms /    22 tokens (    5.25 ms per token,   190.30 tokens per second)

llama_perf_context_print:        eval time =    4092.57 ms /   233 runs   (   17.56 ms per token,    56.93 tokens per second)

llama_perf_context_print:       total time =    4644.53 ms /   255 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 48, 'completion_tokens': 78, 'total_tokens': 126}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 43, 'completion_tokens': 136, 'total_tokens': 179}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 112, 'completion_tokens': 17, 'total_tokens': 129}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 62, 'completion_tokens': 32, 'total_tokens': 94}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 73, 'completion_tokens': 34, 'total_tokens': 107}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 80, 'completion_tokens': 47, 'total_tokens': 127}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 49, 'completion_tokens': 18, 'total_tokens': 67}}
llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     156.35 ms /    48 tokens (    3.26 ms per token,   307.00 tokens per second)

llama_perf_context_print:        eval time =     695.87 ms /    42 runs   (   16.57 ms per token,    60.36 tokens per second)

llama_perf_context_print:       total time =     921.56 ms /    90 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     173.71 ms /    57 tokens (    3.05 ms per token,   328.13 tokens per second)

llama_perf_context_print:        eval time =     468.60 ms /    27 runs   (   17.36 ms per token,    57.62 tokens per second)

llama_perf_context_print:       total time =     689.06 ms /    84 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     184.97 ms /    60 tokens (    3.08 ms per token,   324.38 tokens per second)

llama_perf_context_print:        eval time =    1366.38 ms /    78 runs   (   17.52 ms per token,    57.08 tokens per second)

llama_perf_context_print:       total time =    1686.21 ms /   138 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     250.79 ms /   101 tokens (    2.48 ms per token,   402.73 tokens per second)

llama_perf_context_print:        eval time =    1277.27 ms /    73 runs   (   17.50 ms per token,    57.15 tokens per second)

llama_perf_context_print:       total time =    1653.62 ms /   174 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     169.60 ms /    49 tokens (    3.46 ms per token,   288.91 tokens per second)

llama_perf_context_print:        eval time =    3784.09 ms /   214 runs   (   17.68 ms per token,    56.55 tokens per second)

llama_perf_context_print:       total time =    4350.98 ms /   263 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 45, 'completion_tokens': 233, 'total_tokens': 278}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 71, 'completion_tokens': 42, 'total_tokens': 113}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 83, 'completion_tokens': 27, 'total_tokens': 110}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 86, 'completion_tokens': 78, 'total_tokens': 164}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 124, 'completion_tokens': 73, 'total_tokens': 197}}
llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     188.82 ms /    52 tokens (    3.63 ms per token,   275.39 tokens per second)

llama_perf_context_print:        eval time =    3147.42 ms /   180 runs   (   17.49 ms per token,    57.19 tokens per second)

llama_perf_context_print:       total time =    3659.21 ms /   232 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     129.27 ms /    33 tokens (    3.92 ms per token,   255.28 tokens per second)

llama_perf_context_print:        eval time =    1407.70 ms /    83 runs   (   16.96 ms per token,    58.96 tokens per second)

llama_perf_context_print:       total time =    1678.90 ms /   116 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     150.15 ms /    44 tokens (    3.41 ms per token,   293.05 tokens per second)

llama_perf_context_print:        eval time =    2815.60 ms /   162 runs   (   17.38 ms per token,    57.54 tokens per second)

llama_perf_context_print:       total time =    3254.75 ms /   206 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     251.37 ms /    74 tokens (    3.40 ms per token,   294.39 tokens per second)

llama_perf_context_print:        eval time =    4503.91 ms /   252 runs   (   17.87 ms per token,    55.95 tokens per second)

llama_perf_context_print:       total time =    5231.23 ms /   326 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 72, 'completion_tokens': 214, 'total_tokens': 286}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 75, 'completion_tokens': 180, 'total_tokens': 255}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 56, 'completion_tokens': 83, 'total_tokens': 139}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 67, 'completion_tokens': 162, 'total_tokens': 229}}
llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     149.41 ms /    41 tokens (    3.64 ms per token,   274.41 tokens per second)

llama_perf_context_print:        eval time =    2279.96 ms /   135 runs   (   16.89 ms per token,    59.21 tokens per second)

llama_perf_context_print:       total time =    2662.43 ms /   176 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     141.27 ms /    28 tokens (    5.05 ms per token,   198.20 tokens per second)

llama_perf_context_print:        eval time =    1100.21 ms /    66 runs   (   16.67 ms per token,    59.99 tokens per second)

llama_perf_context_print:       total time =    1355.01 ms /    94 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     152.44 ms /    42 tokens (    3.63 ms per token,   275.52 tokens per second)

llama_perf_context_print:        eval time =    3978.23 ms /   227 runs   (   17.53 ms per token,    57.06 tokens per second)

llama_perf_context_print:       total time =    4549.98 ms /   269 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     130.26 ms /    34 tokens (    3.83 ms per token,   261.02 tokens per second)

llama_perf_context_print:        eval time =    3034.14 ms /   175 runs   (   17.34 ms per token,    57.68 tokens per second)

llama_perf_context_print:       total time =    3478.49 ms /   209 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     125.65 ms /    27 tokens (    4.65 ms per token,   214.88 tokens per second)

llama_perf_context_print:        eval time =    1748.94 ms /   103 runs   (   16.98 ms per token,    58.89 tokens per second)

llama_perf_context_print:       total time =    2051.60 ms /   130 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     119.59 ms /    19 tokens (    6.29 ms per token,   158.88 tokens per second)

llama_perf_context_print:        eval time =     250.08 ms /    15 runs   (   16.67 ms per token,    59.98 tokens per second)

llama_perf_context_print:       total time =     396.22 ms /    34 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 97, 'completion_tokens': 252, 'total_tokens': 349}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 64, 'completion_tokens': 135, 'total_tokens': 199}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 51, 'completion_tokens': 66, 'total_tokens': 117}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 65, 'completion_tokens': 227, 'total_tokens': 292}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 57, 'completion_tokens': 175, 'total_tokens': 232}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 50, 'completion_tokens': 103, 'total_tokens': 153}}
llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     113.52 ms /    24 tokens (    4.73 ms per token,   211.41 tokens per second)

llama_perf_context_print:        eval time =    2329.84 ms /   138 runs   (   16.88 ms per token,    59.23 tokens per second)

llama_perf_context_print:       total time =    2685.00 ms /   162 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     136.41 ms /    30 tokens (    4.55 ms per token,   219.93 tokens per second)

llama_perf_context_print:        eval time =    1498.70 ms /    89 runs   (   16.84 ms per token,    59.38 tokens per second)

llama_perf_context_print:       total time =    1786.92 ms /   119 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     117.38 ms /    18 tokens (    6.52 ms per token,   153.35 tokens per second)

llama_perf_context_print:        eval time =     865.08 ms /    53 runs   (   16.32 ms per token,    61.27 tokens per second)

llama_perf_context_print:       total time =    1072.81 ms /    71 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     105.79 ms /    14 tokens (    7.56 ms per token,   132.34 tokens per second)

llama_perf_context_print:        eval time =     241.01 ms /    14 runs   (   17.21 ms per token,    58.09 tokens per second)

llama_perf_context_print:       total time =     371.83 ms /    28 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     109.26 ms /    17 tokens (    6.43 ms per token,   155.59 tokens per second)

llama_perf_context_print:        eval time =   19530.71 ms /   999 runs   (   19.55 ms per token,    51.15 tokens per second)

llama_perf_context_print:       total time =   22325.77 ms /  1016 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 43, 'completion_tokens': 15, 'total_tokens': 58}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 47, 'completion_tokens': 138, 'total_tokens': 185}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 53, 'completion_tokens': 89, 'total_tokens': 142}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 41, 'completion_tokens': 53, 'total_tokens': 94}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 37, 'completion_tokens': 14, 'total_tokens': 51}}
llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     104.89 ms /    13 tokens (    8.07 ms per token,   123.93 tokens per second)

llama_perf_context_print:        eval time =     359.28 ms /    22 runs   (   16.33 ms per token,    61.23 tokens per second)

llama_perf_context_print:       total time =     503.76 ms /    35 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     114.25 ms /    21 tokens (    5.44 ms per token,   183.81 tokens per second)

llama_perf_context_print:        eval time =     366.02 ms /    22 runs   (   16.64 ms per token,    60.11 tokens per second)

llama_perf_context_print:       total time =     518.56 ms /    43 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     110.22 ms /    16 tokens (    6.89 ms per token,   145.16 tokens per second)

llama_perf_context_print:        eval time =    1608.83 ms /    96 runs   (   16.76 ms per token,    59.67 tokens per second)

llama_perf_context_print:       total time =    1883.05 ms /   112 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     127.26 ms /    33 tokens (    3.86 ms per token,   259.31 tokens per second)

llama_perf_context_print:        eval time =     942.76 ms /    57 runs   (   16.54 ms per token,    60.46 tokens per second)

llama_perf_context_print:       total time =    1166.10 ms /    90 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     239.61 ms /    91 tokens (    2.63 ms per token,   379.79 tokens per second)

llama_perf_context_print:        eval time =     895.46 ms /    51 runs   (   17.56 ms per token,    56.95 tokens per second)

llama_perf_context_print:       total time =    1223.51 ms /   142 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     276.47 ms /   109 tokens (    2.54 ms per token,   394.25 tokens per second)

llama_perf_context_print:        eval time =    1136.53 ms /    65 runs   (   17.49 ms per token,    57.19 tokens per second)

llama_perf_context_print:       total time =    1528.22 ms /   174 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     297.63 ms /   131 tokens (    2.27 ms per token,   440.15 tokens per second)

llama_perf_context_print:        eval time =    1901.18 ms /   107 runs   (   17.77 ms per token,    56.28 tokens per second)

llama_perf_context_print:       total time =    2388.64 ms /   238 tokens

logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 40, 'completion_tokens': 1000, 'total_tokens': 1040}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 36, 'completion_tokens': 22, 'total_tokens': 58}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 44, 'completion_tokens': 22, 'total_tokens': 66}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 39, 'completion_tokens': 96, 'total_tokens': 135}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 56, 'completion_tokens': 57, 'total_tokens': 113}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 114, 'completion_tokens': 51, 'total_tokens': 165}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 132, 'completion_tokens': 65, 'total_tokens': 197}}
llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     283.23 ms /   115 tokens (    2.46 ms per token,   406.03 tokens per second)

llama_perf_context_print:        eval time =    1692.79 ms /    96 runs   (   17.63 ms per token,    56.71 tokens per second)

llama_perf_context_print:       total time =    2146.15 ms /   211 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     209.43 ms /    74 tokens (    2.83 ms per token,   353.33 tokens per second)

llama_perf_context_print:        eval time =     193.53 ms /    11 runs   (   17.59 ms per token,    56.84 tokens per second)

llama_perf_context_print:       total time =     424.49 ms /    85 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     389.95 ms /   135 tokens (    2.89 ms per token,   346.20 tokens per second)

llama_perf_context_print:        eval time =    1715.35 ms /    97 runs   (   17.68 ms per token,    56.55 tokens per second)

llama_perf_context_print:       total time =    2276.49 ms /   232 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     249.19 ms /    96 tokens (    2.60 ms per token,   385.25 tokens per second)

llama_perf_context_print:        eval time =    1271.15 ms /    73 runs   (   17.41 ms per token,    57.43 tokens per second)

llama_perf_context_print:       total time =    1648.44 ms /   169 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 154, 'completion_tokens': 107, 'total_tokens': 261}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 138, 'completion_tokens': 96, 'total_tokens': 234}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 97, 'completion_tokens': 11, 'total_tokens': 108}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 158, 'completion_tokens': 97, 'total_tokens': 255}}
llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     119.27 ms /    26 tokens (    4.59 ms per token,   217.99 tokens per second)

llama_perf_context_print:        eval time =    1765.38 ms /   105 runs   (   16.81 ms per token,    59.48 tokens per second)

llama_perf_context_print:       total time =    2075.65 ms /   131 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     188.32 ms /    62 tokens (    3.04 ms per token,   329.23 tokens per second)

llama_perf_context_print:        eval time =    1132.79 ms /    66 runs   (   17.16 ms per token,    58.26 tokens per second)

llama_perf_context_print:       total time =    1436.97 ms /   128 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     122.37 ms /    20 tokens (    6.12 ms per token,   163.44 tokens per second)

llama_perf_context_print:        eval time =     855.99 ms /    53 runs   (   16.15 ms per token,    61.92 tokens per second)

llama_perf_context_print:       total time =    1070.55 ms /    73 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     150.28 ms /    41 tokens (    3.67 ms per token,   272.82 tokens per second)

llama_perf_context_print:        eval time =     346.13 ms /    21 runs   (   16.48 ms per token,    60.67 tokens per second)

llama_perf_context_print:       total time =     534.04 ms /    62 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     176.01 ms /    57 tokens (    3.09 ms per token,   323.85 tokens per second)

llama_perf_context_print:        eval time =     604.41 ms /    35 runs   (   17.27 ms per token,    57.91 tokens per second)

llama_perf_context_print:       total time =     842.56 ms /    92 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     171.55 ms /    50 tokens (    3.43 ms per token,   291.46 tokens per second)

llama_perf_context_print:        eval time =    5576.12 ms /   308 runs   (   18.10 ms per token,    55.24 tokens per second)

llama_perf_context_print:       total time =    6358.14 ms /   358 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 119, 'completion_tokens': 73, 'total_tokens': 192}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 49, 'completion_tokens': 105, 'total_tokens': 154}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 85, 'completion_tokens': 66, 'total_tokens': 151}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 43, 'completion_tokens': 53, 'total_tokens': 96}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 64, 'completion_tokens': 21, 'total_tokens': 85}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 80, 'completion_tokens': 35, 'total_tokens': 115}}
llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     208.53 ms /    74 tokens (    2.82 ms per token,   354.87 tokens per second)

llama_perf_context_print:        eval time =     815.43 ms /    47 runs   (   17.35 ms per token,    57.64 tokens per second)

llama_perf_context_print:       total time =    1106.21 ms /   121 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     185.76 ms /    65 tokens (    2.86 ms per token,   349.91 tokens per second)

llama_perf_context_print:        eval time =     661.47 ms /    39 runs   (   16.96 ms per token,    58.96 tokens per second)

llama_perf_context_print:       total time =     915.29 ms /   104 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     179.91 ms /    59 tokens (    3.05 ms per token,   327.94 tokens per second)

llama_perf_context_print:        eval time =     484.40 ms /    29 runs   (   16.70 ms per token,    59.87 tokens per second)

llama_perf_context_print:       total time =     715.08 ms /    88 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     172.65 ms /    50 tokens (    3.45 ms per token,   289.61 tokens per second)

llama_perf_context_print:        eval time =     749.00 ms /    46 runs   (   16.28 ms per token,    61.42 tokens per second)

llama_perf_context_print:       total time =     998.86 ms /    96 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     164.06 ms /    52 tokens (    3.16 ms per token,   316.95 tokens per second)

llama_perf_context_print:        eval time =     496.29 ms /    29 runs   (   17.11 ms per token,    58.43 tokens per second)

llama_perf_context_print:       total time =     712.49 ms /    81 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     154.16 ms /    43 tokens (    3.59 ms per token,   278.94 tokens per second)

llama_perf_context_print:        eval time =     572.02 ms /    35 runs   (   16.34 ms per token,    61.19 tokens per second)

llama_perf_context_print:       total time =     787.39 ms /    78 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     185.48 ms /    59 tokens (    3.14 ms per token,   318.09 tokens per second)

llama_perf_context_print:        eval time =     919.30 ms /    53 runs   (   17.35 ms per token,    57.65 tokens per second)

llama_perf_context_print:       total time =    1197.01 ms /   112 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     100.79 ms /    13 tokens (    7.75 ms per token,   128.98 tokens per second)

llama_perf_context_print:        eval time =     532.46 ms /    34 runs   (   15.66 ms per token,    63.86 tokens per second)

llama_perf_context_print:       total time =     690.72 ms /    47 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 73, 'completion_tokens': 308, 'total_tokens': 381}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 97, 'completion_tokens': 47, 'total_tokens': 144}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 88, 'completion_tokens': 39, 'total_tokens': 127}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 82, 'completion_tokens': 29, 'total_tokens': 111}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 73, 'completion_tokens': 46, 'total_tokens': 119}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 75, 'completion_tokens': 29, 'total_tokens': 104}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 66, 'completion_tokens': 35, 'total_tokens': 101}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 82, 'completion_tokens': 53, 'total_tokens': 135}}
llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     204.25 ms /    54 tokens (    3.78 ms per token,   264.38 tokens per second)

llama_perf_context_print:        eval time =     246.12 ms /    15 runs   (   16.41 ms per token,    60.95 tokens per second)

llama_perf_context_print:       total time =     477.14 ms /    69 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     107.72 ms /    15 tokens (    7.18 ms per token,   139.26 tokens per second)

llama_perf_context_print:        eval time =     531.69 ms /    33 runs   (   16.11 ms per token,    62.07 tokens per second)

llama_perf_context_print:       total time =     697.37 ms /    48 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     149.60 ms /    45 tokens (    3.32 ms per token,   300.80 tokens per second)

llama_perf_context_print:        eval time =     412.41 ms /    25 runs   (   16.50 ms per token,    60.62 tokens per second)

llama_perf_context_print:       total time =     605.48 ms /    70 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     111.23 ms /    19 tokens (    5.85 ms per token,   170.82 tokens per second)

llama_perf_context_print:        eval time =    7605.07 ms /   425 runs   (   17.89 ms per token,    55.88 tokens per second)

llama_perf_context_print:       total time =    8629.28 ms /   444 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     154.33 ms /    46 tokens (    3.36 ms per token,   298.06 tokens per second)

llama_perf_context_print:        eval time =     289.86 ms /    17 runs   (   17.05 ms per token,    58.65 tokens per second)

llama_perf_context_print:       total time =     475.37 ms /    63 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 36, 'completion_tokens': 34, 'total_tokens': 70}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 77, 'completion_tokens': 15, 'total_tokens': 92}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 38, 'completion_tokens': 33, 'total_tokens': 71}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 68, 'completion_tokens': 25, 'total_tokens': 93}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 42, 'completion_tokens': 425, 'total_tokens': 467}}
llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     110.65 ms /    15 tokens (    7.38 ms per token,   135.56 tokens per second)

llama_perf_context_print:        eval time =     200.38 ms /    12 runs   (   16.70 ms per token,    59.89 tokens per second)

llama_perf_context_print:       total time =     333.95 ms /    27 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     219.15 ms /    68 tokens (    3.22 ms per token,   310.29 tokens per second)

llama_perf_context_print:        eval time =     844.67 ms /    49 runs   (   17.24 ms per token,    58.01 tokens per second)

llama_perf_context_print:       total time =    1148.77 ms /   117 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     115.45 ms /    18 tokens (    6.41 ms per token,   155.91 tokens per second)

llama_perf_context_print:        eval time =     289.70 ms /    17 runs   (   17.04 ms per token,    58.68 tokens per second)

llama_perf_context_print:       total time =     435.86 ms /    35 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     174.88 ms /    59 tokens (    2.96 ms per token,   337.37 tokens per second)

llama_perf_context_print:        eval time =     201.67 ms /    12 runs   (   16.81 ms per token,    59.50 tokens per second)

llama_perf_context_print:       total time =     399.47 ms /    71 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     152.32 ms /    42 tokens (    3.63 ms per token,   275.74 tokens per second)

llama_perf_context_print:        eval time =      56.05 ms /     3 runs   (   18.68 ms per token,    53.53 tokens per second)

llama_perf_context_print:       total time =     216.14 ms /    45 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     132.14 ms /    35 tokens (    3.78 ms per token,   264.87 tokens per second)

llama_perf_context_print:        eval time =     216.69 ms /    13 runs   (   16.67 ms per token,    59.99 tokens per second)

llama_perf_context_print:       total time =     374.03 ms /    48 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     148.95 ms /    38 tokens (    3.92 ms per token,   255.12 tokens per second)

llama_perf_context_print:        eval time =      40.11 ms /     2 runs   (   20.05 ms per token,    49.86 tokens per second)

llama_perf_context_print:       total time =     195.16 ms /    40 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     137.29 ms /    41 tokens (    3.35 ms per token,   298.64 tokens per second)

llama_perf_context_print:        eval time =     706.36 ms /    43 runs   (   16.43 ms per token,    60.88 tokens per second)

llama_perf_context_print:       total time =     918.59 ms /    84 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     164.61 ms /    54 tokens (    3.05 ms per token,   328.05 tokens per second)

llama_perf_context_print:        eval time =     187.81 ms /    11 runs   (   17.07 ms per token,    58.57 tokens per second)

llama_perf_context_print:       total time =     373.46 ms /    65 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     147.66 ms /    41 tokens (    3.60 ms per token,   277.67 tokens per second)

llama_perf_context_print:        eval time =     466.51 ms /    28 runs   (   16.66 ms per token,    60.02 tokens per second)

llama_perf_context_print:       total time =     663.32 ms /    69 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 69, 'completion_tokens': 17, 'total_tokens': 86}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 38, 'completion_tokens': 12, 'total_tokens': 50}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 91, 'completion_tokens': 49, 'total_tokens': 140}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 41, 'completion_tokens': 17, 'total_tokens': 58}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 82, 'completion_tokens': 12, 'total_tokens': 94}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 65, 'completion_tokens': 3, 'total_tokens': 68}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 58, 'completion_tokens': 13, 'total_tokens': 71}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 61, 'completion_tokens': 2, 'total_tokens': 63}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 64, 'completion_tokens': 43, 'total_tokens': 107}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 77, 'completion_tokens': 11, 'total_tokens': 88}}
llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     162.72 ms /    41 tokens (    3.97 ms per token,   251.97 tokens per second)

llama_perf_context_print:        eval time =     265.67 ms /    16 runs   (   16.60 ms per token,    60.23 tokens per second)

llama_perf_context_print:       total time =     457.82 ms /    57 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     148.72 ms /    43 tokens (    3.46 ms per token,   289.14 tokens per second)

llama_perf_context_print:        eval time =     200.02 ms /    12 runs   (   16.67 ms per token,    59.99 tokens per second)

llama_perf_context_print:       total time =     371.12 ms /    55 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     175.41 ms /    43 tokens (    4.08 ms per token,   245.14 tokens per second)

llama_perf_context_print:        eval time =      24.19 ms /     1 runs   (   24.19 ms per token,    41.34 tokens per second)

llama_perf_context_print:       total time =     204.23 ms /    44 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     123.99 ms /    35 tokens (    3.54 ms per token,   282.29 tokens per second)

llama_perf_context_print:        eval time =    1160.71 ms /    65 runs   (   17.86 ms per token,    56.00 tokens per second)

llama_perf_context_print:       total time =    1395.27 ms /   100 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     188.04 ms /    57 tokens (    3.30 ms per token,   303.13 tokens per second)

llama_perf_context_print:        eval time =    3454.56 ms /   196 runs   (   17.63 ms per token,    56.74 tokens per second)

llama_perf_context_print:       total time =    4006.01 ms /   253 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     155.70 ms /    44 tokens (    3.54 ms per token,   282.59 tokens per second)

llama_perf_context_print:        eval time =    2386.72 ms /   138 runs   (   17.30 ms per token,    57.82 tokens per second)

llama_perf_context_print:       total time =    2792.74 ms /   182 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     182.57 ms /    51 tokens (    3.58 ms per token,   279.34 tokens per second)

llama_perf_context_print:        eval time =    1619.53 ms /    95 runs   (   17.05 ms per token,    58.66 tokens per second)

llama_perf_context_print:       total time =    1970.75 ms /   146 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 64, 'completion_tokens': 28, 'total_tokens': 92}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 64, 'completion_tokens': 16, 'total_tokens': 80}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 66, 'completion_tokens': 12, 'total_tokens': 78}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 66, 'completion_tokens': 1, 'total_tokens': 67}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 58, 'completion_tokens': 65, 'total_tokens': 123}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 80, 'completion_tokens': 196, 'total_tokens': 276}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 67, 'completion_tokens': 138, 'total_tokens': 205}}
llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     211.56 ms /    76 tokens (    2.78 ms per token,   359.24 tokens per second)

llama_perf_context_print:        eval time =   13595.78 ms /   716 runs   (   18.99 ms per token,    52.66 tokens per second)

llama_perf_context_print:       total time =   15597.79 ms /   792 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 74, 'completion_tokens': 95, 'total_tokens': 169}}
llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     165.94 ms /    56 tokens (    2.96 ms per token,   337.46 tokens per second)

llama_perf_context_print:        eval time =     466.53 ms /    28 runs   (   16.66 ms per token,    60.02 tokens per second)

llama_perf_context_print:       total time =     682.59 ms /    84 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     212.75 ms /    58 tokens (    3.67 ms per token,   272.62 tokens per second)

llama_perf_context_print:        eval time =    3604.89 ms /   206 runs   (   17.50 ms per token,    57.14 tokens per second)

llama_perf_context_print:       total time =    4208.21 ms /   264 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     177.90 ms /    56 tokens (    3.18 ms per token,   314.79 tokens per second)

llama_perf_context_print:        eval time =    4072.62 ms /   231 runs   (   17.63 ms per token,    56.72 tokens per second)

llama_perf_context_print:       total time =    4694.43 ms /   287 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     122.08 ms /    29 tokens (    4.21 ms per token,   237.54 tokens per second)

llama_perf_context_print:        eval time =     116.15 ms /     6 runs   (   19.36 ms per token,    51.66 tokens per second)

llama_perf_context_print:       total time =     251.27 ms /    35 tokens

llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     196.41 ms /    55 tokens (    3.57 ms per token,   280.03 tokens per second)

llama_perf_context_print:        eval time =   19565.24 ms /   999 runs   (   19.58 ms per token,    51.06 tokens per second)

llama_perf_context_print:       total time =   22466.99 ms /  1054 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 99, 'completion_tokens': 716, 'total_tokens': 815}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 79, 'completion_tokens': 28, 'total_tokens': 107}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 82, 'completion_tokens': 206, 'total_tokens': 288}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 79, 'completion_tokens': 231, 'total_tokens': 310}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 52, 'completion_tokens': 6, 'total_tokens': 58}}
llama_perf_context_print:        load time =     240.25 ms

llama_perf_context_print: prompt eval time =     281.65 ms /   107 tokens (    2.63 ms per token,   379.91 tokens per second)

llama_perf_context_print:        eval time =    1323.96 ms /    76 runs   (   17.42 ms per token,    57.40 tokens per second)

llama_perf_context_print:       total time =    1740.29 ms /   183 tokens

logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 78, 'completion_tokens': 1000, 'total_tokens': 1078}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 130, 'completion_tokens': 76, 'total_tokens': 206}}
