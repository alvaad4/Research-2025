llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     361.76 ms /    17 tokens (   21.28 ms per token,    46.99 tokens per second)

llama_perf_context_print:        eval time =     157.52 ms /     7 runs   (   22.50 ms per token,    44.44 tokens per second)

llama_perf_context_print:       total time =     548.71 ms /    24 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     117.74 ms /    20 tokens (    5.89 ms per token,   169.86 tokens per second)

llama_perf_context_print:        eval time =    3713.56 ms /   173 runs   (   21.47 ms per token,    46.59 tokens per second)

llama_perf_context_print:       total time =    4424.72 ms /   193 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     106.26 ms /    20 tokens (    5.31 ms per token,   188.22 tokens per second)

llama_perf_context_print:        eval time =    3137.52 ms /   146 runs   (   21.49 ms per token,    46.53 tokens per second)

llama_perf_context_print:       total time =    3736.20 ms /   166 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     137.08 ms /    26 tokens (    5.27 ms per token,   189.67 tokens per second)

llama_perf_context_print:        eval time =     616.20 ms /    29 runs   (   21.25 ms per token,    47.06 tokens per second)

llama_perf_context_print:       total time =     851.88 ms /    55 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     106.52 ms /    16 tokens (    6.66 ms per token,   150.20 tokens per second)

llama_perf_context_print:        eval time =    2293.37 ms /   108 runs   (   21.23 ms per token,    47.09 tokens per second)

llama_perf_context_print:       total time =    2759.55 ms /   124 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 17, 'completion_tokens': 7, 'total_tokens': 24}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 24, 'completion_tokens': 173, 'total_tokens': 197}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 24, 'completion_tokens': 146, 'total_tokens': 170}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 30, 'completion_tokens': 29, 'total_tokens': 59}}
llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     115.04 ms /    14 tokens (    8.22 ms per token,   121.70 tokens per second)

llama_perf_context_print:        eval time =    2303.86 ms /   107 runs   (   21.53 ms per token,    46.44 tokens per second)

llama_perf_context_print:       total time =    2774.18 ms /   121 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     107.89 ms /    17 tokens (    6.35 ms per token,   157.57 tokens per second)

llama_perf_context_print:        eval time =     518.71 ms /    24 runs   (   21.61 ms per token,    46.27 tokens per second)

llama_perf_context_print:       total time =     708.40 ms /    41 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     110.01 ms /    14 tokens (    7.86 ms per token,   127.26 tokens per second)

llama_perf_context_print:        eval time =     218.94 ms /    10 runs   (   21.89 ms per token,    45.67 tokens per second)

llama_perf_context_print:       total time =     366.41 ms /    24 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     113.80 ms /    24 tokens (    4.74 ms per token,   210.91 tokens per second)

llama_perf_context_print:        eval time =     582.61 ms /    27 runs   (   21.58 ms per token,    46.34 tokens per second)

llama_perf_context_print:       total time =     788.45 ms /    51 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     112.84 ms /    20 tokens (    5.64 ms per token,   177.25 tokens per second)

llama_perf_context_print:        eval time =    1755.45 ms /    82 runs   (   21.41 ms per token,    46.71 tokens per second)

llama_perf_context_print:       total time =    2145.16 ms /   102 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     270.18 ms /    93 tokens (    2.91 ms per token,   344.22 tokens per second)

llama_perf_context_print:        eval time =     589.04 ms /    27 runs   (   21.82 ms per token,    45.84 tokens per second)

llama_perf_context_print:       total time =     952.09 ms /   120 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     133.65 ms /    38 tokens (    3.52 ms per token,   284.33 tokens per second)

llama_perf_context_print:        eval time =     310.06 ms /    14 runs   (   22.15 ms per token,    45.15 tokens per second)

llama_perf_context_print:       total time =     494.82 ms /    52 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     146.84 ms /    47 tokens (    3.12 ms per token,   320.07 tokens per second)

llama_perf_context_print:        eval time =    2998.77 ms /   134 runs   (   22.38 ms per token,    44.68 tokens per second)

llama_perf_context_print:       total time =    3593.62 ms /   181 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 20, 'completion_tokens': 108, 'total_tokens': 128}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 18, 'completion_tokens': 107, 'total_tokens': 125}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 21, 'completion_tokens': 24, 'total_tokens': 45}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 18, 'completion_tokens': 10, 'total_tokens': 28}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 28, 'completion_tokens': 27, 'total_tokens': 55}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 24, 'completion_tokens': 82, 'total_tokens': 106}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 97, 'completion_tokens': 27, 'total_tokens': 124}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 42, 'completion_tokens': 14, 'total_tokens': 56}}
llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     160.79 ms /    55 tokens (    2.92 ms per token,   342.07 tokens per second)

llama_perf_context_print:        eval time =    3802.70 ms /   175 runs   (   21.73 ms per token,    46.02 tokens per second)

llama_perf_context_print:       total time =    4553.79 ms /   230 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     112.98 ms /    25 tokens (    4.52 ms per token,   221.27 tokens per second)

llama_perf_context_print:        eval time =     720.06 ms /    33 runs   (   21.82 ms per token,    45.83 tokens per second)

llama_perf_context_print:       total time =     943.90 ms /    58 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     123.56 ms /    23 tokens (    5.37 ms per token,   186.14 tokens per second)

llama_perf_context_print:        eval time =    1057.27 ms /    48 runs   (   22.03 ms per token,    45.40 tokens per second)

llama_perf_context_print:       total time =    1346.99 ms /    71 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     152.43 ms /    50 tokens (    3.05 ms per token,   328.02 tokens per second)

llama_perf_context_print:        eval time =     138.82 ms /     6 runs   (   23.14 ms per token,    43.22 tokens per second)

llama_perf_context_print:       total time =     316.49 ms /    56 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     163.84 ms /    56 tokens (    2.93 ms per token,   341.79 tokens per second)

llama_perf_context_print:        eval time =     157.61 ms /     7 runs   (   22.52 ms per token,    44.41 tokens per second)

llama_perf_context_print:       total time =     348.98 ms /    63 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     173.35 ms /    68 tokens (    2.55 ms per token,   392.27 tokens per second)

llama_perf_context_print:        eval time =     161.65 ms /     7 runs   (   23.09 ms per token,    43.30 tokens per second)

llama_perf_context_print:       total time =     362.61 ms /    75 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     241.86 ms /   102 tokens (    2.37 ms per token,   421.73 tokens per second)

llama_perf_context_print:        eval time =     548.75 ms /    25 runs   (   21.95 ms per token,    45.56 tokens per second)

llama_perf_context_print:       total time =     880.62 ms /   127 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     182.86 ms /    53 tokens (    3.45 ms per token,   289.84 tokens per second)

llama_perf_context_print:        eval time =    4423.32 ms /   206 runs   (   21.47 ms per token,    46.57 tokens per second)

llama_perf_context_print:       total time =    5306.38 ms /   259 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 51, 'completion_tokens': 134, 'total_tokens': 185}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 59, 'completion_tokens': 175, 'total_tokens': 234}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 29, 'completion_tokens': 33, 'total_tokens': 62}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 27, 'completion_tokens': 48, 'total_tokens': 75}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 54, 'completion_tokens': 6, 'total_tokens': 60}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 63, 'completion_tokens': 7, 'total_tokens': 70}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 75, 'completion_tokens': 7, 'total_tokens': 82}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 106, 'completion_tokens': 25, 'total_tokens': 131}}
llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     155.08 ms /    54 tokens (    2.87 ms per token,   348.21 tokens per second)

llama_perf_context_print:        eval time =    3830.75 ms /   178 runs   (   21.52 ms per token,    46.47 tokens per second)

llama_perf_context_print:       total time =    4589.40 ms /   232 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     127.51 ms /    34 tokens (    3.75 ms per token,   266.64 tokens per second)

llama_perf_context_print:        eval time =    1571.97 ms /    73 runs   (   21.53 ms per token,    46.44 tokens per second)

llama_perf_context_print:       total time =    1944.08 ms /   107 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     146.49 ms /    47 tokens (    3.12 ms per token,   320.84 tokens per second)

llama_perf_context_print:        eval time =    8889.99 ms /   406 runs   (   21.90 ms per token,    45.67 tokens per second)

llama_perf_context_print:       total time =   10538.04 ms /   453 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     212.08 ms /    75 tokens (    2.83 ms per token,   353.64 tokens per second)

llama_perf_context_print:        eval time =    7729.89 ms /   352 runs   (   21.96 ms per token,    45.54 tokens per second)

llama_perf_context_print:       total time =    9223.21 ms /   427 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 57, 'completion_tokens': 206, 'total_tokens': 263}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 58, 'completion_tokens': 178, 'total_tokens': 236}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 38, 'completion_tokens': 73, 'total_tokens': 111}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 51, 'completion_tokens': 406, 'total_tokens': 457}}
llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     144.03 ms /    45 tokens (    3.20 ms per token,   312.43 tokens per second)

llama_perf_context_print:        eval time =    3973.72 ms /   185 runs   (   21.48 ms per token,    46.56 tokens per second)

llama_perf_context_print:       total time =    4750.46 ms /   230 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     146.03 ms /    31 tokens (    4.71 ms per token,   212.28 tokens per second)

llama_perf_context_print:        eval time =    3228.47 ms /   150 runs   (   21.52 ms per token,    46.46 tokens per second)

llama_perf_context_print:       total time =    3876.15 ms /   181 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     168.95 ms /    44 tokens (    3.84 ms per token,   260.43 tokens per second)

llama_perf_context_print:        eval time =    4686.11 ms /   215 runs   (   21.80 ms per token,    45.88 tokens per second)

llama_perf_context_print:       total time =    5599.46 ms /   259 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     154.20 ms /    35 tokens (    4.41 ms per token,   226.98 tokens per second)

llama_perf_context_print:        eval time =    3393.21 ms /   156 runs   (   21.75 ms per token,    45.97 tokens per second)

llama_perf_context_print:       total time =    4078.21 ms /   191 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     138.72 ms /    29 tokens (    4.78 ms per token,   209.06 tokens per second)

llama_perf_context_print:        eval time =    5960.34 ms /   278 runs   (   21.44 ms per token,    46.64 tokens per second)

llama_perf_context_print:       total time =    7073.44 ms /   307 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 79, 'completion_tokens': 352, 'total_tokens': 431}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 49, 'completion_tokens': 185, 'total_tokens': 234}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 35, 'completion_tokens': 150, 'total_tokens': 185}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 48, 'completion_tokens': 215, 'total_tokens': 263}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 39, 'completion_tokens': 156, 'total_tokens': 195}}
llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     107.96 ms /    22 tokens (    4.91 ms per token,   203.78 tokens per second)

llama_perf_context_print:        eval time =     395.65 ms /    18 runs   (   21.98 ms per token,    45.49 tokens per second)

llama_perf_context_print:       total time =     567.25 ms /    40 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     125.91 ms /    29 tokens (    4.34 ms per token,   230.33 tokens per second)

llama_perf_context_print:        eval time =    3754.34 ms /   175 runs   (   21.45 ms per token,    46.61 tokens per second)

llama_perf_context_print:       total time =    4470.13 ms /   204 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     131.33 ms /    35 tokens (    3.75 ms per token,   266.50 tokens per second)

llama_perf_context_print:        eval time =    3947.75 ms /   184 runs   (   21.46 ms per token,    46.61 tokens per second)

llama_perf_context_print:       total time =    4709.87 ms /   219 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     106.84 ms /    19 tokens (    5.62 ms per token,   177.84 tokens per second)

llama_perf_context_print:        eval time =    3073.66 ms /   142 runs   (   21.65 ms per token,    46.20 tokens per second)

llama_perf_context_print:       total time =    3656.89 ms /   161 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     116.35 ms /    15 tokens (    7.76 ms per token,   128.92 tokens per second)

llama_perf_context_print:        eval time =    3090.96 ms /   143 runs   (   21.62 ms per token,    46.26 tokens per second)

llama_perf_context_print:       total time =    3696.90 ms /   158 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     103.68 ms /    17 tokens (    6.10 ms per token,   163.97 tokens per second)

llama_perf_context_print:        eval time =   22199.24 ms /   999 runs   (   22.22 ms per token,    45.00 tokens per second)

llama_perf_context_print:       total time =   26697.31 ms /  1016 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 33, 'completion_tokens': 278, 'total_tokens': 311}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 27, 'completion_tokens': 18, 'total_tokens': 45}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 33, 'completion_tokens': 175, 'total_tokens': 208}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 39, 'completion_tokens': 184, 'total_tokens': 223}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 23, 'completion_tokens': 142, 'total_tokens': 165}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 19, 'completion_tokens': 143, 'total_tokens': 162}}
llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     110.38 ms /    16 tokens (    6.90 ms per token,   144.96 tokens per second)

llama_perf_context_print:        eval time =    1433.79 ms /    67 runs   (   21.40 ms per token,    46.73 tokens per second)

llama_perf_context_print:       total time =    1768.46 ms /    83 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     111.42 ms /    22 tokens (    5.06 ms per token,   197.46 tokens per second)

llama_perf_context_print:        eval time =     454.64 ms /    21 runs   (   21.65 ms per token,    46.19 tokens per second)

llama_perf_context_print:       total time =     639.20 ms /    43 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     102.40 ms /    17 tokens (    6.02 ms per token,   166.02 tokens per second)

llama_perf_context_print:        eval time =   22159.06 ms /   999 runs   (   22.18 ms per token,    45.08 tokens per second)

llama_perf_context_print:       total time =   26595.34 ms /  1016 tokens

logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 21, 'completion_tokens': 1000, 'total_tokens': 1021}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 26, 'completion_tokens': 21, 'total_tokens': 47}}
llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     162.59 ms /    37 tokens (    4.39 ms per token,   227.57 tokens per second)

llama_perf_context_print:        eval time =    1722.71 ms /    80 runs   (   21.53 ms per token,    46.44 tokens per second)

llama_perf_context_print:       total time =    2153.34 ms /   117 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     255.37 ms /    90 tokens (    2.84 ms per token,   352.43 tokens per second)

llama_perf_context_print:        eval time =     431.47 ms /    20 runs   (   21.57 ms per token,    46.35 tokens per second)

llama_perf_context_print:       total time =     755.33 ms /   110 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     273.83 ms /   114 tokens (    2.40 ms per token,   416.31 tokens per second)

llama_perf_context_print:        eval time =    1055.45 ms /    49 runs   (   21.54 ms per token,    46.43 tokens per second)

llama_perf_context_print:       total time =    1490.77 ms /   163 tokens

logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 21, 'completion_tokens': 1000, 'total_tokens': 1021}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 41, 'completion_tokens': 80, 'total_tokens': 121}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 94, 'completion_tokens': 20, 'total_tokens': 114}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 118, 'completion_tokens': 49, 'total_tokens': 167}}
llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     320.41 ms /   131 tokens (    2.45 ms per token,   408.85 tokens per second)

llama_perf_context_print:        eval time =    1003.24 ms /    46 runs   (   21.81 ms per token,    45.85 tokens per second)

llama_perf_context_print:       total time =    1476.47 ms /   177 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     317.29 ms /   119 tokens (    2.67 ms per token,   375.05 tokens per second)

llama_perf_context_print:        eval time =    2596.98 ms /   120 runs   (   21.64 ms per token,    46.21 tokens per second)

llama_perf_context_print:       total time =    3317.84 ms /   239 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     227.24 ms /    76 tokens (    2.99 ms per token,   334.45 tokens per second)

llama_perf_context_print:        eval time =    1522.91 ms /    70 runs   (   21.76 ms per token,    45.96 tokens per second)

llama_perf_context_print:       total time =    1986.55 ms /   146 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     311.84 ms /   129 tokens (    2.42 ms per token,   413.67 tokens per second)

llama_perf_context_print:        eval time =    2546.87 ms /   118 runs   (   21.58 ms per token,    46.33 tokens per second)

llama_perf_context_print:       total time =    3252.55 ms /   247 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     228.08 ms /   104 tokens (    2.19 ms per token,   455.99 tokens per second)

llama_perf_context_print:        eval time =    1317.68 ms /    61 runs   (   21.60 ms per token,    46.29 tokens per second)

llama_perf_context_print:       total time =    1748.53 ms /   165 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 135, 'completion_tokens': 46, 'total_tokens': 181}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 123, 'completion_tokens': 120, 'total_tokens': 243}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 80, 'completion_tokens': 70, 'total_tokens': 150}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 133, 'completion_tokens': 118, 'total_tokens': 251}}
llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     123.38 ms /    24 tokens (    5.14 ms per token,   194.52 tokens per second)

llama_perf_context_print:        eval time =    2260.74 ms /   105 runs   (   21.53 ms per token,    46.45 tokens per second)

llama_perf_context_print:       total time =    2743.76 ms /   129 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     207.89 ms /    66 tokens (    3.15 ms per token,   317.48 tokens per second)

llama_perf_context_print:        eval time =     651.16 ms /    30 runs   (   21.71 ms per token,    46.07 tokens per second)

llama_perf_context_print:       total time =     965.36 ms /    96 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     114.92 ms /    19 tokens (    6.05 ms per token,   165.34 tokens per second)

llama_perf_context_print:        eval time =    1015.96 ms /    47 runs   (   21.62 ms per token,    46.26 tokens per second)

llama_perf_context_print:       total time =    1289.13 ms /    66 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     166.05 ms /    41 tokens (    4.05 ms per token,   246.91 tokens per second)

llama_perf_context_print:        eval time =     789.71 ms /    36 runs   (   21.94 ms per token,    45.59 tokens per second)

llama_perf_context_print:       total time =    1078.96 ms /    77 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     167.97 ms /    59 tokens (    2.85 ms per token,   351.26 tokens per second)

llama_perf_context_print:        eval time =    1056.32 ms /    49 runs   (   21.56 ms per token,    46.39 tokens per second)

llama_perf_context_print:       total time =    1386.99 ms /   108 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     157.22 ms /    55 tokens (    2.86 ms per token,   349.83 tokens per second)

llama_perf_context_print:        eval time =     740.57 ms /    34 runs   (   21.78 ms per token,    45.91 tokens per second)

llama_perf_context_print:       total time =    1010.84 ms /    89 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     196.69 ms /    78 tokens (    2.52 ms per token,   396.57 tokens per second)

llama_perf_context_print:        eval time =    1308.49 ms /    61 runs   (   21.45 ms per token,    46.62 tokens per second)

llama_perf_context_print:       total time =    1707.56 ms /   139 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 108, 'completion_tokens': 61, 'total_tokens': 169}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 28, 'completion_tokens': 105, 'total_tokens': 133}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 70, 'completion_tokens': 30, 'total_tokens': 100}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 23, 'completion_tokens': 47, 'total_tokens': 70}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 45, 'completion_tokens': 36, 'total_tokens': 81}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 63, 'completion_tokens': 49, 'total_tokens': 112}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 59, 'completion_tokens': 34, 'total_tokens': 93}}
llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     200.59 ms /    68 tokens (    2.95 ms per token,   339.00 tokens per second)

llama_perf_context_print:        eval time =     943.90 ms /    44 runs   (   21.45 ms per token,    46.62 tokens per second)

llama_perf_context_print:       total time =    1292.19 ms /   112 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     174.99 ms /    60 tokens (    2.92 ms per token,   342.87 tokens per second)

llama_perf_context_print:        eval time =     775.90 ms /    36 runs   (   21.55 ms per token,    46.40 tokens per second)

llama_perf_context_print:       total time =    1071.24 ms /    96 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     158.82 ms /    55 tokens (    2.89 ms per token,   346.30 tokens per second)

llama_perf_context_print:        eval time =    1050.02 ms /    48 runs   (   21.88 ms per token,    45.71 tokens per second)

llama_perf_context_print:       total time =    1367.67 ms /   103 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     170.13 ms /    56 tokens (    3.04 ms per token,   329.15 tokens per second)

llama_perf_context_print:        eval time =     930.65 ms /    43 runs   (   21.64 ms per token,    46.20 tokens per second)

llama_perf_context_print:       total time =    1246.75 ms /    99 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     144.06 ms /    46 tokens (    3.13 ms per token,   319.32 tokens per second)

llama_perf_context_print:        eval time =     865.96 ms /    40 runs   (   21.65 ms per token,    46.19 tokens per second)

llama_perf_context_print:       total time =    1144.36 ms /    86 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     170.72 ms /    68 tokens (    2.51 ms per token,   398.32 tokens per second)

llama_perf_context_print:        eval time =    1071.94 ms /    50 runs   (   21.44 ms per token,    46.64 tokens per second)

llama_perf_context_print:       total time =    1408.92 ms /   118 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     112.40 ms /    14 tokens (    8.03 ms per token,   124.55 tokens per second)

llama_perf_context_print:        eval time =     994.10 ms /    46 runs   (   21.61 ms per token,    46.27 tokens per second)

llama_perf_context_print:       total time =    1257.82 ms /    60 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     173.51 ms /    59 tokens (    2.94 ms per token,   340.04 tokens per second)

llama_perf_context_print:        eval time =     828.58 ms /    38 runs   (   21.80 ms per token,    45.86 tokens per second)

llama_perf_context_print:       total time =    1129.40 ms /    97 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 82, 'completion_tokens': 61, 'total_tokens': 143}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 72, 'completion_tokens': 44, 'total_tokens': 116}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 64, 'completion_tokens': 36, 'total_tokens': 100}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 59, 'completion_tokens': 48, 'total_tokens': 107}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 60, 'completion_tokens': 43, 'total_tokens': 103}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 50, 'completion_tokens': 40, 'total_tokens': 90}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 72, 'completion_tokens': 50, 'total_tokens': 122}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 18, 'completion_tokens': 46, 'total_tokens': 64}}
llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     108.22 ms /    16 tokens (    6.76 ms per token,   147.85 tokens per second)

llama_perf_context_print:        eval time =     240.66 ms /    11 runs   (   21.88 ms per token,    45.71 tokens per second)

llama_perf_context_print:       total time =     388.26 ms /    27 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     149.84 ms /    47 tokens (    3.19 ms per token,   313.66 tokens per second)

llama_perf_context_print:        eval time =     646.85 ms /    30 runs   (   21.56 ms per token,    46.38 tokens per second)

llama_perf_context_print:       total time =     898.61 ms /    77 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     118.45 ms /    20 tokens (    5.92 ms per token,   168.85 tokens per second)

llama_perf_context_print:        eval time =   22583.53 ms /   999 runs   (   22.61 ms per token,    44.24 tokens per second)

llama_perf_context_print:       total time =   27131.82 ms /  1019 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     151.19 ms /    49 tokens (    3.09 ms per token,   324.10 tokens per second)

llama_perf_context_print:        eval time =     411.83 ms /    19 runs   (   21.68 ms per token,    46.14 tokens per second)

llama_perf_context_print:       total time =     629.61 ms /    68 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =      98.98 ms /    16 tokens (    6.19 ms per token,   161.66 tokens per second)

llama_perf_context_print:        eval time =     852.70 ms /    39 runs   (   21.86 ms per token,    45.74 tokens per second)

llama_perf_context_print:       total time =    1086.82 ms /    55 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     185.16 ms /    75 tokens (    2.47 ms per token,   405.06 tokens per second)

llama_perf_context_print:        eval time =    2171.14 ms /   101 runs   (   21.50 ms per token,    46.52 tokens per second)

llama_perf_context_print:       total time =    2690.27 ms /   176 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     104.11 ms /    20 tokens (    5.21 ms per token,   192.10 tokens per second)

llama_perf_context_print:        eval time =    5019.70 ms /   230 runs   (   21.82 ms per token,    45.82 tokens per second)

llama_perf_context_print:       total time =    5930.09 ms /   250 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     215.26 ms /    64 tokens (    3.36 ms per token,   297.31 tokens per second)

llama_perf_context_print:        eval time =     309.13 ms /    14 runs   (   22.08 ms per token,    45.29 tokens per second)

llama_perf_context_print:       total time =     575.21 ms /    78 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 63, 'completion_tokens': 38, 'total_tokens': 101}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 20, 'completion_tokens': 11, 'total_tokens': 31}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 51, 'completion_tokens': 30, 'total_tokens': 81}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 53, 'completion_tokens': 19, 'total_tokens': 72}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 20, 'completion_tokens': 39, 'total_tokens': 59}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 79, 'completion_tokens': 101, 'total_tokens': 180}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 24, 'completion_tokens': 230, 'total_tokens': 254}}
llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     154.65 ms /    43 tokens (    3.60 ms per token,   278.05 tokens per second)

llama_perf_context_print:        eval time =     698.10 ms /    32 runs   (   21.82 ms per token,    45.84 tokens per second)

llama_perf_context_print:       total time =     960.81 ms /    75 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     156.42 ms /    35 tokens (    4.47 ms per token,   223.76 tokens per second)

llama_perf_context_print:        eval time =    1011.77 ms /    47 runs   (   21.53 ms per token,    46.45 tokens per second)

llama_perf_context_print:       total time =    1328.98 ms /    82 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     156.15 ms /    39 tokens (    4.00 ms per token,   249.76 tokens per second)

llama_perf_context_print:        eval time =    1368.48 ms /    64 runs   (   21.38 ms per token,    46.77 tokens per second)

llama_perf_context_print:       total time =    1736.96 ms /   103 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     161.29 ms /    40 tokens (    4.03 ms per token,   248.01 tokens per second)

llama_perf_context_print:        eval time =    2415.61 ms /   112 runs   (   21.57 ms per token,    46.37 tokens per second)

llama_perf_context_print:       total time =    2955.91 ms /   152 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     173.61 ms /    57 tokens (    3.05 ms per token,   328.32 tokens per second)

llama_perf_context_print:        eval time =    1329.76 ms /    62 runs   (   21.45 ms per token,    46.62 tokens per second)

llama_perf_context_print:       total time =    1708.79 ms /   119 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     157.31 ms /    44 tokens (    3.58 ms per token,   279.71 tokens per second)

llama_perf_context_print:        eval time =     802.26 ms /    37 runs   (   21.68 ms per token,    46.12 tokens per second)

llama_perf_context_print:       total time =    1085.98 ms /    81 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     136.10 ms /    41 tokens (    3.32 ms per token,   301.25 tokens per second)

llama_perf_context_print:        eval time =    3213.40 ms /   149 runs   (   21.57 ms per token,    46.37 tokens per second)

llama_perf_context_print:       total time =    3849.52 ms /   190 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 68, 'completion_tokens': 14, 'total_tokens': 82}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 47, 'completion_tokens': 32, 'total_tokens': 79}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 39, 'completion_tokens': 47, 'total_tokens': 86}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 43, 'completion_tokens': 64, 'total_tokens': 107}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 44, 'completion_tokens': 112, 'total_tokens': 156}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 61, 'completion_tokens': 62, 'total_tokens': 123}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 48, 'completion_tokens': 37, 'total_tokens': 85}}
llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     169.64 ms /    44 tokens (    3.86 ms per token,   259.37 tokens per second)

llama_perf_context_print:        eval time =     509.40 ms /    24 runs   (   21.23 ms per token,    47.11 tokens per second)

llama_perf_context_print:       total time =     760.00 ms /    68 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     143.76 ms /    45 tokens (    3.19 ms per token,   313.02 tokens per second)

llama_perf_context_print:        eval time =     834.12 ms /    39 runs   (   21.39 ms per token,    46.76 tokens per second)

llama_perf_context_print:       total time =    1109.91 ms /    84 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     154.71 ms /    38 tokens (    4.07 ms per token,   245.63 tokens per second)

llama_perf_context_print:        eval time =    1007.90 ms /    47 runs   (   21.44 ms per token,    46.63 tokens per second)

llama_perf_context_print:       total time =    1321.42 ms /    85 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     164.74 ms /    61 tokens (    2.70 ms per token,   370.29 tokens per second)

llama_perf_context_print:        eval time =   10060.41 ms /   434 runs   (   23.18 ms per token,    43.14 tokens per second)

llama_perf_context_print:       total time =   11854.06 ms /   495 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 45, 'completion_tokens': 149, 'total_tokens': 194}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 48, 'completion_tokens': 24, 'total_tokens': 72}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 49, 'completion_tokens': 39, 'total_tokens': 88}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 42, 'completion_tokens': 47, 'total_tokens': 89}}
llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     168.08 ms /    48 tokens (    3.50 ms per token,   285.58 tokens per second)

llama_perf_context_print:        eval time =    3112.61 ms /   145 runs   (   21.47 ms per token,    46.58 tokens per second)

llama_perf_context_print:       total time =    3771.99 ms /   193 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     153.29 ms /    55 tokens (    2.79 ms per token,   358.80 tokens per second)

llama_perf_context_print:        eval time =    4082.27 ms /   190 runs   (   21.49 ms per token,    46.54 tokens per second)

llama_perf_context_print:       total time =    4884.81 ms /   245 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     206.91 ms /    79 tokens (    2.62 ms per token,   381.80 tokens per second)

llama_perf_context_print:        eval time =   22779.95 ms /   999 runs   (   22.80 ms per token,    43.85 tokens per second)

llama_perf_context_print:       total time =   27254.69 ms /  1078 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 65, 'completion_tokens': 434, 'total_tokens': 499}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 52, 'completion_tokens': 145, 'total_tokens': 197}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 59, 'completion_tokens': 190, 'total_tokens': 249}}
llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     164.13 ms /    62 tokens (    2.65 ms per token,   377.75 tokens per second)

llama_perf_context_print:        eval time =   11752.73 ms /   526 runs   (   22.34 ms per token,    44.76 tokens per second)

llama_perf_context_print:       total time =   13940.94 ms /   588 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     175.05 ms /    63 tokens (    2.78 ms per token,   359.89 tokens per second)

llama_perf_context_print:        eval time =    1389.64 ms /    64 runs   (   21.71 ms per token,    46.06 tokens per second)

llama_perf_context_print:       total time =    1780.38 ms /   127 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     197.87 ms /    60 tokens (    3.30 ms per token,   303.22 tokens per second)

llama_perf_context_print:        eval time =   23807.42 ms /   999 runs   (   23.83 ms per token,    41.96 tokens per second)

llama_perf_context_print:       total time =   28387.04 ms /  1059 tokens

logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 83, 'completion_tokens': 1000, 'total_tokens': 1083}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 66, 'completion_tokens': 526, 'total_tokens': 592}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 68, 'completion_tokens': 64, 'total_tokens': 132}}
llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     136.41 ms /    33 tokens (    4.13 ms per token,   241.92 tokens per second)

llama_perf_context_print:        eval time =     699.73 ms /    31 runs   (   22.57 ms per token,    44.30 tokens per second)

llama_perf_context_print:       total time =     946.34 ms /    64 tokens

llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     173.24 ms /    58 tokens (    2.99 ms per token,   334.80 tokens per second)

llama_perf_context_print:        eval time =   23862.86 ms /   999 runs   (   23.89 ms per token,    41.86 tokens per second)

llama_perf_context_print:       total time =   28408.95 ms /  1057 tokens

logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 64, 'completion_tokens': 1000, 'total_tokens': 1064}}
llama_perf_context_print:        load time =     361.93 ms

llama_perf_context_print: prompt eval time =     271.22 ms /   111 tokens (    2.44 ms per token,   409.27 tokens per second)

llama_perf_context_print:        eval time =    7879.77 ms /   326 runs   (   24.17 ms per token,    41.37 tokens per second)

llama_perf_context_print:       total time =    9331.49 ms /   437 tokens

logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 62, 'completion_tokens': 1000, 'total_tokens': 1062}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 115, 'completion_tokens': 326, 'total_tokens': 441}}
