llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     148.95 ms /    35 tokens (    4.26 ms per token,   234.98 tokens per second)

llama_perf_context_print:        eval time =     176.15 ms /     7 runs   (   25.16 ms per token,    39.74 tokens per second)

llama_perf_context_print:       total time =     332.26 ms /    42 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =      93.02 ms /    19 tokens (    4.90 ms per token,   204.27 tokens per second)

llama_perf_context_print:        eval time =     167.52 ms /     6 runs   (   27.92 ms per token,    35.82 tokens per second)

llama_perf_context_print:       total time =     266.45 ms /    25 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =      93.90 ms /    19 tokens (    4.94 ms per token,   202.34 tokens per second)

llama_perf_context_print:        eval time =     153.82 ms /     6 runs   (   25.64 ms per token,    39.01 tokens per second)

llama_perf_context_print:       total time =     253.66 ms /    25 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     120.11 ms /    26 tokens (    4.62 ms per token,   216.47 tokens per second)

llama_perf_context_print:        eval time =     501.74 ms /    20 runs   (   25.09 ms per token,    39.86 tokens per second)

llama_perf_context_print:       total time =     639.53 ms /    46 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =      91.91 ms /    13 tokens (    7.07 ms per token,   141.44 tokens per second)

llama_perf_context_print:        eval time =     276.15 ms /    11 runs   (   25.10 ms per token,    39.83 tokens per second)

llama_perf_context_print:       total time =     378.13 ms /    24 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =      82.88 ms /    14 tokens (    5.92 ms per token,   168.92 tokens per second)

llama_perf_context_print:        eval time =     888.51 ms /    34 runs   (   26.13 ms per token,    38.27 tokens per second)

llama_perf_context_print:       total time =    1003.01 ms /    48 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =      79.29 ms /    16 tokens (    4.96 ms per token,   201.79 tokens per second)

llama_perf_context_print:        eval time =     804.98 ms /    32 runs   (   25.16 ms per token,    39.75 tokens per second)

llama_perf_context_print:       total time =     915.99 ms /    48 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =      85.61 ms /    13 tokens (    6.59 ms per token,   151.85 tokens per second)

llama_perf_context_print:        eval time =     193.53 ms /     8 runs   (   24.19 ms per token,    41.34 tokens per second)

llama_perf_context_print:       total time =     287.71 ms /    21 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     130.82 ms /    25 tokens (    5.23 ms per token,   191.11 tokens per second)

llama_perf_context_print:        eval time =    2111.55 ms /    84 runs   (   25.14 ms per token,    39.78 tokens per second)

llama_perf_context_print:       total time =    2329.15 ms /   109 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 35, 'completion_tokens': 7, 'total_tokens': 42}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 42, 'completion_tokens': 6, 'total_tokens': 48}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 42, 'completion_tokens': 6, 'total_tokens': 48}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 49, 'completion_tokens': 20, 'total_tokens': 69}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 36, 'completion_tokens': 11, 'total_tokens': 47}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 37, 'completion_tokens': 34, 'total_tokens': 71}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 39, 'completion_tokens': 32, 'total_tokens': 71}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 36, 'completion_tokens': 8, 'total_tokens': 44}}
llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     101.88 ms /    20 tokens (    5.09 ms per token,   196.31 tokens per second)

llama_perf_context_print:        eval time =    3117.50 ms /   122 runs   (   25.55 ms per token,    39.13 tokens per second)

llama_perf_context_print:       total time =    3348.77 ms /   142 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     425.81 ms /    89 tokens (    4.78 ms per token,   209.02 tokens per second)

llama_perf_context_print:        eval time =     623.16 ms /    24 runs   (   25.97 ms per token,    38.51 tokens per second)

llama_perf_context_print:       total time =    1073.38 ms /   113 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     196.77 ms /    39 tokens (    5.05 ms per token,   198.20 tokens per second)

llama_perf_context_print:        eval time =     810.07 ms /    32 runs   (   25.31 ms per token,    39.50 tokens per second)

llama_perf_context_print:       total time =    1038.89 ms /    71 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     238.37 ms /    50 tokens (    4.77 ms per token,   209.76 tokens per second)

llama_perf_context_print:        eval time =    3450.63 ms /   134 runs   (   25.75 ms per token,    38.83 tokens per second)

llama_perf_context_print:       total time =    3837.38 ms /   184 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     280.15 ms /    57 tokens (    4.91 ms per token,   203.46 tokens per second)

llama_perf_context_print:        eval time =    1179.99 ms /    47 runs   (   25.11 ms per token,    39.83 tokens per second)

llama_perf_context_print:       total time =    1507.42 ms /   104 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     141.64 ms /    26 tokens (    5.45 ms per token,   183.56 tokens per second)

llama_perf_context_print:        eval time =     461.73 ms /    18 runs   (   25.65 ms per token,    38.98 tokens per second)

llama_perf_context_print:       total time =     621.59 ms /    44 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     121.86 ms /    22 tokens (    5.54 ms per token,   180.53 tokens per second)

llama_perf_context_print:        eval time =    3810.24 ms /   151 runs   (   25.23 ms per token,    39.63 tokens per second)

llama_perf_context_print:       total time =    4097.45 ms /   173 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 48, 'completion_tokens': 84, 'total_tokens': 132}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 43, 'completion_tokens': 122, 'total_tokens': 165}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 112, 'completion_tokens': 24, 'total_tokens': 136}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 62, 'completion_tokens': 32, 'total_tokens': 94}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 73, 'completion_tokens': 134, 'total_tokens': 207}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 80, 'completion_tokens': 47, 'total_tokens': 127}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 49, 'completion_tokens': 18, 'total_tokens': 67}}
llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     221.91 ms /    48 tokens (    4.62 ms per token,   216.30 tokens per second)

llama_perf_context_print:        eval time =     655.93 ms /    24 runs   (   27.33 ms per token,    36.59 tokens per second)

llama_perf_context_print:       total time =     902.08 ms /    72 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     272.99 ms /    57 tokens (    4.79 ms per token,   208.80 tokens per second)

llama_perf_context_print:        eval time =     715.22 ms /    28 runs   (   25.54 ms per token,    39.15 tokens per second)

llama_perf_context_print:       total time =    1016.51 ms /    85 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     286.61 ms /    60 tokens (    4.78 ms per token,   209.35 tokens per second)

llama_perf_context_print:        eval time =    1958.83 ms /    77 runs   (   25.44 ms per token,    39.31 tokens per second)

llama_perf_context_print:       total time =    2328.76 ms /   137 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     491.16 ms /   101 tokens (    4.86 ms per token,   205.64 tokens per second)

llama_perf_context_print:        eval time =    1891.94 ms /    74 runs   (   25.57 ms per token,    39.11 tokens per second)

llama_perf_context_print:       total time =    2459.01 ms /   175 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     232.16 ms /    49 tokens (    4.74 ms per token,   211.06 tokens per second)

llama_perf_context_print:        eval time =    5237.68 ms /   205 runs   (   25.55 ms per token,    39.14 tokens per second)

llama_perf_context_print:       total time =    5701.00 ms /   254 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 45, 'completion_tokens': 151, 'total_tokens': 196}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 71, 'completion_tokens': 24, 'total_tokens': 95}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 83, 'completion_tokens': 28, 'total_tokens': 111}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 86, 'completion_tokens': 77, 'total_tokens': 163}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 124, 'completion_tokens': 74, 'total_tokens': 198}}
llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     258.54 ms /    52 tokens (    4.97 ms per token,   201.13 tokens per second)

llama_perf_context_print:        eval time =    4334.63 ms /   170 runs   (   25.50 ms per token,    39.22 tokens per second)

llama_perf_context_print:       total time =    4780.04 ms /   222 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     160.43 ms /    33 tokens (    4.86 ms per token,   205.69 tokens per second)

llama_perf_context_print:        eval time =    2207.41 ms /    87 runs   (   25.37 ms per token,    39.41 tokens per second)

llama_perf_context_print:       total time =    2457.44 ms /   120 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     219.66 ms /    44 tokens (    4.99 ms per token,   200.31 tokens per second)

llama_perf_context_print:        eval time =    5423.55 ms /   212 runs   (   25.58 ms per token,    39.09 tokens per second)

llama_perf_context_print:       total time =    5889.19 ms /   256 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     378.31 ms /    74 tokens (    5.11 ms per token,   195.61 tokens per second)

llama_perf_context_print:        eval time =   26919.37 ms /   999 runs   (   26.95 ms per token,    37.11 tokens per second)

llama_perf_context_print:       total time =   29195.78 ms /  1073 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 72, 'completion_tokens': 205, 'total_tokens': 277}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 75, 'completion_tokens': 170, 'total_tokens': 245}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 56, 'completion_tokens': 87, 'total_tokens': 143}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 67, 'completion_tokens': 212, 'total_tokens': 279}}
llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     206.73 ms /    41 tokens (    5.04 ms per token,   198.33 tokens per second)

llama_perf_context_print:        eval time =    3398.89 ms /   134 runs   (   25.36 ms per token,    39.42 tokens per second)

llama_perf_context_print:       total time =    3749.16 ms /   175 tokens

logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 97, 'completion_tokens': 1000, 'total_tokens': 1097}}
llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     165.40 ms /    28 tokens (    5.91 ms per token,   169.28 tokens per second)

llama_perf_context_print:        eval time =    3280.92 ms /   128 runs   (   25.63 ms per token,    39.01 tokens per second)

llama_perf_context_print:       total time =    3582.37 ms /   156 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     211.55 ms /    42 tokens (    5.04 ms per token,   198.54 tokens per second)

llama_perf_context_print:        eval time =    3425.26 ms /   134 runs   (   25.56 ms per token,    39.12 tokens per second)

llama_perf_context_print:       total time =    3780.10 ms /   176 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     172.14 ms /    34 tokens (    5.06 ms per token,   197.51 tokens per second)

llama_perf_context_print:        eval time =    3656.10 ms /   144 runs   (   25.39 ms per token,    39.39 tokens per second)

llama_perf_context_print:       total time =    3987.37 ms /   178 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     149.89 ms /    27 tokens (    5.55 ms per token,   180.13 tokens per second)

llama_perf_context_print:        eval time =    3578.85 ms /   140 runs   (   25.56 ms per token,    39.12 tokens per second)

llama_perf_context_print:       total time =    3883.32 ms /   167 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     110.96 ms /    19 tokens (    5.84 ms per token,   171.23 tokens per second)

llama_perf_context_print:        eval time =     369.68 ms /    15 runs   (   24.65 ms per token,    40.58 tokens per second)

llama_perf_context_print:       total time =     499.98 ms /    34 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     122.45 ms /    24 tokens (    5.10 ms per token,   196.00 tokens per second)

llama_perf_context_print:        eval time =    4394.76 ms /   172 runs   (   25.55 ms per token,    39.14 tokens per second)

llama_perf_context_print:       total time =    4706.43 ms /   196 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 64, 'completion_tokens': 134, 'total_tokens': 198}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 51, 'completion_tokens': 128, 'total_tokens': 179}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 65, 'completion_tokens': 134, 'total_tokens': 199}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 57, 'completion_tokens': 144, 'total_tokens': 201}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 50, 'completion_tokens': 140, 'total_tokens': 190}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 43, 'completion_tokens': 15, 'total_tokens': 58}}
llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     162.06 ms /    30 tokens (    5.40 ms per token,   185.12 tokens per second)

llama_perf_context_print:        eval time =    2191.71 ms /    85 runs   (   25.78 ms per token,    38.78 tokens per second)

llama_perf_context_print:       total time =    2445.08 ms /   115 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     102.09 ms /    18 tokens (    5.67 ms per token,   176.31 tokens per second)

llama_perf_context_print:        eval time =    1302.19 ms /    51 runs   (   25.53 ms per token,    39.16 tokens per second)

llama_perf_context_print:       total time =    1455.47 ms /    69 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =      93.10 ms /    14 tokens (    6.65 ms per token,   150.37 tokens per second)

llama_perf_context_print:        eval time =    2234.90 ms /    89 runs   (   25.11 ms per token,    39.82 tokens per second)

llama_perf_context_print:       total time =    2420.07 ms /   103 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =      89.77 ms /    17 tokens (    5.28 ms per token,   189.38 tokens per second)

llama_perf_context_print:        eval time =    8618.71 ms /   336 runs   (   25.65 ms per token,    38.98 tokens per second)

llama_perf_context_print:       total time =    9132.68 ms /   353 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 47, 'completion_tokens': 172, 'total_tokens': 219}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 53, 'completion_tokens': 85, 'total_tokens': 138}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 41, 'completion_tokens': 51, 'total_tokens': 92}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 37, 'completion_tokens': 89, 'total_tokens': 126}}
llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =      86.34 ms /    13 tokens (    6.64 ms per token,   150.57 tokens per second)

llama_perf_context_print:        eval time =     538.75 ms /    21 runs   (   25.65 ms per token,    38.98 tokens per second)

llama_perf_context_print:       total time =     646.61 ms /    34 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     109.90 ms /    21 tokens (    5.23 ms per token,   191.09 tokens per second)

llama_perf_context_print:        eval time =     694.26 ms /    27 runs   (   25.71 ms per token,    38.89 tokens per second)

llama_perf_context_print:       total time =     831.07 ms /    48 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =      81.97 ms /    16 tokens (    5.12 ms per token,   195.20 tokens per second)

llama_perf_context_print:        eval time =   14050.28 ms /   537 runs   (   26.16 ms per token,    38.22 tokens per second)

llama_perf_context_print:       total time =   14918.48 ms /   553 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 40, 'completion_tokens': 336, 'total_tokens': 376}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 36, 'completion_tokens': 21, 'total_tokens': 57}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 44, 'completion_tokens': 27, 'total_tokens': 71}}
llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     163.88 ms /    33 tokens (    4.97 ms per token,   201.37 tokens per second)

llama_perf_context_print:        eval time =    1738.31 ms /    69 runs   (   25.19 ms per token,    39.69 tokens per second)

llama_perf_context_print:       total time =    1972.32 ms /   102 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     441.07 ms /    91 tokens (    4.85 ms per token,   206.31 tokens per second)

llama_perf_context_print:        eval time =    1347.57 ms /    53 runs   (   25.43 ms per token,    39.33 tokens per second)

llama_perf_context_print:       total time =    1842.28 ms /   144 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     532.81 ms /   109 tokens (    4.89 ms per token,   204.58 tokens per second)

llama_perf_context_print:        eval time =    1665.17 ms /    65 runs   (   25.62 ms per token,    39.04 tokens per second)

llama_perf_context_print:       total time =    2264.28 ms /   174 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 39, 'completion_tokens': 537, 'total_tokens': 576}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 56, 'completion_tokens': 69, 'total_tokens': 125}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 114, 'completion_tokens': 53, 'total_tokens': 167}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 132, 'completion_tokens': 65, 'total_tokens': 197}}
llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     612.22 ms /   131 tokens (    4.67 ms per token,   213.97 tokens per second)

llama_perf_context_print:        eval time =    2747.10 ms /   107 runs   (   25.67 ms per token,    38.95 tokens per second)

llama_perf_context_print:       total time =    3471.94 ms /   238 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     585.85 ms /   115 tokens (    5.09 ms per token,   196.30 tokens per second)

llama_perf_context_print:        eval time =    1995.47 ms /    78 runs   (   25.58 ms per token,    39.09 tokens per second)

llama_perf_context_print:       total time =    2661.74 ms /   193 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     358.30 ms /    74 tokens (    4.84 ms per token,   206.53 tokens per second)

llama_perf_context_print:        eval time =     467.88 ms /    18 runs   (   25.99 ms per token,    38.47 tokens per second)

llama_perf_context_print:       total time =     844.60 ms /    92 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     612.25 ms /   131 tokens (    4.67 ms per token,   213.97 tokens per second)

llama_perf_context_print:        eval time =    2472.90 ms /    97 runs   (   25.49 ms per token,    39.23 tokens per second)

llama_perf_context_print:       total time =    3186.42 ms /   228 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     443.32 ms /    96 tokens (    4.62 ms per token,   216.55 tokens per second)

llama_perf_context_print:        eval time =    1788.67 ms /    70 runs   (   25.55 ms per token,    39.14 tokens per second)

llama_perf_context_print:       total time =    2303.72 ms /   166 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 154, 'completion_tokens': 107, 'total_tokens': 261}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 138, 'completion_tokens': 78, 'total_tokens': 216}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 97, 'completion_tokens': 18, 'total_tokens': 115}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 154, 'completion_tokens': 97, 'total_tokens': 251}}
llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     142.57 ms /    26 tokens (    5.48 ms per token,   182.37 tokens per second)

llama_perf_context_print:        eval time =    2319.91 ms /    90 runs   (   25.78 ms per token,    38.79 tokens per second)

llama_perf_context_print:       total time =    2555.57 ms /   116 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     301.60 ms /    62 tokens (    4.86 ms per token,   205.57 tokens per second)

llama_perf_context_print:        eval time =    1852.47 ms /    73 runs   (   25.38 ms per token,    39.41 tokens per second)

llama_perf_context_print:       total time =    2229.71 ms /   135 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     104.11 ms /    20 tokens (    5.21 ms per token,   192.10 tokens per second)

llama_perf_context_print:        eval time =    1542.86 ms /    61 runs   (   25.29 ms per token,    39.54 tokens per second)

llama_perf_context_print:       total time =    1708.79 ms /    81 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     205.72 ms /    41 tokens (    5.02 ms per token,   199.30 tokens per second)

llama_perf_context_print:        eval time =     515.51 ms /    21 runs   (   24.55 ms per token,    40.74 tokens per second)

llama_perf_context_print:       total time =     742.64 ms /    62 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     294.38 ms /    57 tokens (    5.16 ms per token,   193.63 tokens per second)

llama_perf_context_print:        eval time =     876.16 ms /    35 runs   (   25.03 ms per token,    39.95 tokens per second)

llama_perf_context_print:       total time =    1205.68 ms /    92 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     243.55 ms /    50 tokens (    4.87 ms per token,   205.30 tokens per second)

llama_perf_context_print:        eval time =    7346.65 ms /   285 runs   (   25.78 ms per token,    38.79 tokens per second)

llama_perf_context_print:       total time =    7941.74 ms /   335 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 119, 'completion_tokens': 70, 'total_tokens': 189}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 49, 'completion_tokens': 90, 'total_tokens': 139}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 85, 'completion_tokens': 73, 'total_tokens': 158}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 43, 'completion_tokens': 61, 'total_tokens': 104}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 64, 'completion_tokens': 21, 'total_tokens': 85}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 80, 'completion_tokens': 35, 'total_tokens': 115}}
llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     361.81 ms /    74 tokens (    4.89 ms per token,   204.53 tokens per second)

llama_perf_context_print:        eval time =    1555.48 ms /    61 runs   (   25.50 ms per token,    39.22 tokens per second)

llama_perf_context_print:       total time =    1979.37 ms /   135 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     301.13 ms /    64 tokens (    4.71 ms per token,   212.53 tokens per second)

llama_perf_context_print:        eval time =     712.16 ms /    28 runs   (   25.43 ms per token,    39.32 tokens per second)

llama_perf_context_print:       total time =    1041.52 ms /    92 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     294.85 ms /    59 tokens (    5.00 ms per token,   200.11 tokens per second)

llama_perf_context_print:        eval time =     992.78 ms /    39 runs   (   25.46 ms per token,    39.28 tokens per second)

llama_perf_context_print:       total time =    1326.78 ms /    98 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     238.78 ms /    50 tokens (    4.78 ms per token,   209.40 tokens per second)

llama_perf_context_print:        eval time =     871.73 ms /    33 runs   (   26.42 ms per token,    37.86 tokens per second)

llama_perf_context_print:       total time =    1143.80 ms /    83 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     244.19 ms /    52 tokens (    4.70 ms per token,   212.95 tokens per second)

llama_perf_context_print:        eval time =     733.82 ms /    29 runs   (   25.30 ms per token,    39.52 tokens per second)

llama_perf_context_print:       total time =    1007.12 ms /    81 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 73, 'completion_tokens': 285, 'total_tokens': 358}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 97, 'completion_tokens': 61, 'total_tokens': 158}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 87, 'completion_tokens': 28, 'total_tokens': 115}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 82, 'completion_tokens': 39, 'total_tokens': 121}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 73, 'completion_tokens': 33, 'total_tokens': 106}}
llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     221.48 ms /    43 tokens (    5.15 ms per token,   194.15 tokens per second)

llama_perf_context_print:        eval time =     890.00 ms /    35 runs   (   25.43 ms per token,    39.33 tokens per second)

llama_perf_context_print:       total time =    1146.69 ms /    78 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     320.53 ms /    59 tokens (    5.43 ms per token,   184.07 tokens per second)

llama_perf_context_print:        eval time =     986.39 ms /    39 runs   (   25.29 ms per token,    39.54 tokens per second)

llama_perf_context_print:       total time =    1346.37 ms /    98 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =      87.58 ms /    13 tokens (    6.74 ms per token,   148.43 tokens per second)

llama_perf_context_print:        eval time =     835.31 ms /    33 runs   (   25.31 ms per token,    39.51 tokens per second)

llama_perf_context_print:       total time =     955.89 ms /    46 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     265.21 ms /    54 tokens (    4.91 ms per token,   203.61 tokens per second)

llama_perf_context_print:        eval time =     368.87 ms /    15 runs   (   24.59 ms per token,    40.66 tokens per second)

llama_perf_context_print:       total time =     649.61 ms /    69 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     103.30 ms /    15 tokens (    6.89 ms per token,   145.21 tokens per second)

llama_perf_context_print:        eval time =     804.95 ms /    32 runs   (   25.15 ms per token,    39.75 tokens per second)

llama_perf_context_print:       total time =     940.25 ms /    47 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     228.11 ms /    45 tokens (    5.07 ms per token,   197.27 tokens per second)

llama_perf_context_print:        eval time =     640.13 ms /    25 runs   (   25.61 ms per token,    39.05 tokens per second)

llama_perf_context_print:       total time =     893.72 ms /    70 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     110.36 ms /    19 tokens (    5.81 ms per token,   172.17 tokens per second)

llama_perf_context_print:        eval time =    9167.66 ms /   355 runs   (   25.82 ms per token,    38.72 tokens per second)

llama_perf_context_print:       total time =    9733.46 ms /   374 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 75, 'completion_tokens': 29, 'total_tokens': 104}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 66, 'completion_tokens': 35, 'total_tokens': 101}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 82, 'completion_tokens': 39, 'total_tokens': 121}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 36, 'completion_tokens': 33, 'total_tokens': 69}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 77, 'completion_tokens': 15, 'total_tokens': 92}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 38, 'completion_tokens': 32, 'total_tokens': 70}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 68, 'completion_tokens': 25, 'total_tokens': 93}}
llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     233.19 ms /    46 tokens (    5.07 ms per token,   197.27 tokens per second)

llama_perf_context_print:        eval time =     420.24 ms /    17 runs   (   24.72 ms per token,    40.45 tokens per second)

llama_perf_context_print:       total time =     670.99 ms /    63 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     115.52 ms /    15 tokens (    7.70 ms per token,   129.84 tokens per second)

llama_perf_context_print:        eval time =     818.07 ms /    33 runs   (   24.79 ms per token,    40.34 tokens per second)

llama_perf_context_print:       total time =     966.47 ms /    48 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     336.54 ms /    68 tokens (    4.95 ms per token,   202.06 tokens per second)

llama_perf_context_print:        eval time =    1265.60 ms /    49 runs   (   25.83 ms per token,    38.72 tokens per second)

llama_perf_context_print:       total time =    1655.59 ms /   117 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     101.30 ms /    18 tokens (    5.63 ms per token,   177.69 tokens per second)

llama_perf_context_print:        eval time =     435.60 ms /    17 runs   (   25.62 ms per token,    39.03 tokens per second)

llama_perf_context_print:       total time =     554.24 ms /    35 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     287.59 ms /    58 tokens (    4.96 ms per token,   201.67 tokens per second)

llama_perf_context_print:        eval time =     663.08 ms /    26 runs   (   25.50 ms per token,    39.21 tokens per second)

llama_perf_context_print:       total time =     977.06 ms /    84 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     214.64 ms /    42 tokens (    5.11 ms per token,   195.68 tokens per second)

llama_perf_context_print:        eval time =      77.02 ms /     3 runs   (   25.67 ms per token,    38.95 tokens per second)

llama_perf_context_print:       total time =     295.68 ms /    45 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     172.61 ms /    35 tokens (    4.93 ms per token,   202.77 tokens per second)

llama_perf_context_print:        eval time =     346.58 ms /    14 runs   (   24.76 ms per token,    40.39 tokens per second)

llama_perf_context_print:       total time =     533.73 ms /    49 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 42, 'completion_tokens': 355, 'total_tokens': 397}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 69, 'completion_tokens': 17, 'total_tokens': 86}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 38, 'completion_tokens': 33, 'total_tokens': 71}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 91, 'completion_tokens': 49, 'total_tokens': 140}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 41, 'completion_tokens': 17, 'total_tokens': 58}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 81, 'completion_tokens': 26, 'total_tokens': 107}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 65, 'completion_tokens': 3, 'total_tokens': 68}}
llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     205.31 ms /    38 tokens (    5.40 ms per token,   185.08 tokens per second)

llama_perf_context_print:        eval time =      52.60 ms /     2 runs   (   26.30 ms per token,    38.02 tokens per second)

llama_perf_context_print:       total time =     260.98 ms /    40 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     205.63 ms /    41 tokens (    5.02 ms per token,   199.38 tokens per second)

llama_perf_context_print:        eval time =    1277.72 ms /    51 runs   (   25.05 ms per token,    39.91 tokens per second)

llama_perf_context_print:       total time =    1535.11 ms /    92 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     255.05 ms /    53 tokens (    4.81 ms per token,   207.80 tokens per second)

llama_perf_context_print:        eval time =     194.36 ms /     7 runs   (   27.77 ms per token,    36.02 tokens per second)

llama_perf_context_print:       total time =     457.23 ms /    60 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     202.18 ms /    41 tokens (    4.93 ms per token,   202.79 tokens per second)

llama_perf_context_print:        eval time =     706.28 ms /    28 runs   (   25.22 ms per token,    39.64 tokens per second)

llama_perf_context_print:       total time =     936.87 ms /    69 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     203.62 ms /    41 tokens (    4.97 ms per token,   201.36 tokens per second)

llama_perf_context_print:        eval time =     852.30 ms /    34 runs   (   25.07 ms per token,    39.89 tokens per second)

llama_perf_context_print:       total time =    1090.34 ms /    75 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     220.28 ms /    43 tokens (    5.12 ms per token,   195.20 tokens per second)

llama_perf_context_print:        eval time =     295.89 ms /    12 runs   (   24.66 ms per token,    40.56 tokens per second)

llama_perf_context_print:       total time =     528.94 ms /    55 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     220.57 ms /    43 tokens (    5.13 ms per token,   194.95 tokens per second)

llama_perf_context_print:        eval time =      31.11 ms /     1 runs   (   31.11 ms per token,    32.14 tokens per second)

llama_perf_context_print:       total time =     253.82 ms /    44 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     188.27 ms /    35 tokens (    5.38 ms per token,   185.91 tokens per second)

llama_perf_context_print:        eval time =    1535.75 ms /    60 runs   (   25.60 ms per token,    39.07 tokens per second)

llama_perf_context_print:       total time =    1785.21 ms /    95 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 58, 'completion_tokens': 14, 'total_tokens': 72}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 61, 'completion_tokens': 2, 'total_tokens': 63}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 64, 'completion_tokens': 51, 'total_tokens': 115}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 76, 'completion_tokens': 7, 'total_tokens': 83}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 64, 'completion_tokens': 28, 'total_tokens': 92}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 64, 'completion_tokens': 34, 'total_tokens': 98}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 66, 'completion_tokens': 12, 'total_tokens': 78}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 66, 'completion_tokens': 1, 'total_tokens': 67}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 58, 'completion_tokens': 60, 'total_tokens': 118}}
llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     288.76 ms /    57 tokens (    5.07 ms per token,   197.39 tokens per second)

llama_perf_context_print:        eval time =    2607.03 ms /   103 runs   (   25.31 ms per token,    39.51 tokens per second)

llama_perf_context_print:       total time =    3004.21 ms /   160 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     232.46 ms /    44 tokens (    5.28 ms per token,   189.28 tokens per second)

llama_perf_context_print:        eval time =   11524.12 ms /   443 runs   (   26.01 ms per token,    38.44 tokens per second)

llama_perf_context_print:       total time =   12364.51 ms /   487 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     248.72 ms /    51 tokens (    4.88 ms per token,   205.05 tokens per second)

llama_perf_context_print:        eval time =    1377.41 ms /    55 runs   (   25.04 ms per token,    39.93 tokens per second)

llama_perf_context_print:       total time =    1682.09 ms /   106 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 80, 'completion_tokens': 103, 'total_tokens': 183}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 67, 'completion_tokens': 443, 'total_tokens': 510}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 74, 'completion_tokens': 55, 'total_tokens': 129}}
llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     383.13 ms /    76 tokens (    5.04 ms per token,   198.37 tokens per second)

llama_perf_context_print:        eval time =   15469.38 ms /   588 runs   (   26.31 ms per token,    38.01 tokens per second)

llama_perf_context_print:       total time =   16760.15 ms /   664 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     264.76 ms /    56 tokens (    4.73 ms per token,   211.51 tokens per second)

llama_perf_context_print:        eval time =    1101.70 ms /    43 runs   (   25.62 ms per token,    39.03 tokens per second)

llama_perf_context_print:       total time =    1409.92 ms /    99 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 99, 'completion_tokens': 588, 'total_tokens': 687}}
llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     286.57 ms /    58 tokens (    4.94 ms per token,   202.40 tokens per second)

llama_perf_context_print:        eval time =    4707.43 ms /   183 runs   (   25.72 ms per token,    38.87 tokens per second)

llama_perf_context_print:       total time =    5198.46 ms /   241 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     270.25 ms /    56 tokens (    4.83 ms per token,   207.21 tokens per second)

llama_perf_context_print:        eval time =    5471.22 ms /   214 runs   (   25.57 ms per token,    39.11 tokens per second)

llama_perf_context_print:       total time =    5988.08 ms /   270 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     157.92 ms /    29 tokens (    5.45 ms per token,   183.63 tokens per second)

llama_perf_context_print:        eval time =     145.20 ms /     6 runs   (   24.20 ms per token,    41.32 tokens per second)

llama_perf_context_print:       total time =     309.99 ms /    35 tokens

llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     269.33 ms /    55 tokens (    4.90 ms per token,   204.21 tokens per second)

llama_perf_context_print:        eval time =    7009.53 ms /   272 runs   (   25.77 ms per token,    38.80 tokens per second)

llama_perf_context_print:       total time =    7608.62 ms /   327 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 79, 'completion_tokens': 43, 'total_tokens': 122}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 82, 'completion_tokens': 183, 'total_tokens': 265}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 79, 'completion_tokens': 214, 'total_tokens': 293}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 52, 'completion_tokens': 6, 'total_tokens': 58}}
llama_perf_context_print:        load time =     149.05 ms

llama_perf_context_print: prompt eval time =     515.70 ms /   107 tokens (    4.82 ms per token,   207.49 tokens per second)

llama_perf_context_print:        eval time =    1673.51 ms /    65 runs   (   25.75 ms per token,    38.84 tokens per second)

llama_perf_context_print:       total time =    2255.86 ms /   172 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 78, 'completion_tokens': 272, 'total_tokens': 350}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 130, 'completion_tokens': 65, 'total_tokens': 195}}
