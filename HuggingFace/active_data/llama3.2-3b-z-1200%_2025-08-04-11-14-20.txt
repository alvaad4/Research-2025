llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     696.53 ms /    55 tokens (   12.66 ms per token,    78.96 tokens per second)

llama_perf_context_print:        eval time =     426.63 ms /     7 runs   (   60.95 ms per token,    16.41 tokens per second)

llama_perf_context_print:       total time =    1131.54 ms /    62 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     324.00 ms /    21 tokens (   15.43 ms per token,    64.82 tokens per second)

llama_perf_context_print:        eval time =    2730.64 ms /    44 runs   (   62.06 ms per token,    16.11 tokens per second)

llama_perf_context_print:       total time =    3103.71 ms /    65 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     324.52 ms /    21 tokens (   15.45 ms per token,    64.71 tokens per second)

llama_perf_context_print:        eval time =    4034.23 ms /    65 runs   (   62.07 ms per token,    16.11 tokens per second)

llama_perf_context_print:       total time =    4426.46 ms /    86 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     449.30 ms /    28 tokens (   16.05 ms per token,    62.32 tokens per second)

llama_perf_context_print:        eval time =    1112.65 ms /    18 runs   (   61.81 ms per token,    16.18 tokens per second)

llama_perf_context_print:       total time =    1580.65 ms /    46 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     263.75 ms /    14 tokens (   18.84 ms per token,    53.08 tokens per second)

llama_perf_context_print:        eval time =    5576.75 ms /    90 runs   (   61.96 ms per token,    16.14 tokens per second)

llama_perf_context_print:       total time =    5935.82 ms /   104 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     225.61 ms /    16 tokens (   14.10 ms per token,    70.92 tokens per second)

llama_perf_context_print:        eval time =    3526.75 ms /    56 runs   (   62.98 ms per token,    15.88 tokens per second)

llama_perf_context_print:       total time =    3810.10 ms /    72 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     263.99 ms /    17 tokens (   15.53 ms per token,    64.40 tokens per second)

llama_perf_context_print:        eval time =    2114.28 ms /    34 runs   (   62.18 ms per token,    16.08 tokens per second)

llama_perf_context_print:       total time =    2413.05 ms /    51 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     307.65 ms /    15 tokens (   20.51 ms per token,    48.76 tokens per second)

llama_perf_context_print:        eval time =     490.70 ms /     8 runs   (   61.34 ms per token,    16.30 tokens per second)

llama_perf_context_print:       total time =     807.24 ms /    23 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 55, 'completion_tokens': 7, 'total_tokens': 62}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 62, 'completion_tokens': 44, 'total_tokens': 106}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 62, 'completion_tokens': 65, 'total_tokens': 127}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 69, 'completion_tokens': 18, 'total_tokens': 87}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 55, 'completion_tokens': 90, 'total_tokens': 145}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 57, 'completion_tokens': 56, 'total_tokens': 113}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 58, 'completion_tokens': 34, 'total_tokens': 92}}
llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     437.57 ms /    27 tokens (   16.21 ms per token,    61.70 tokens per second)

llama_perf_context_print:        eval time =     879.46 ms /    14 runs   (   62.82 ms per token,    15.92 tokens per second)

llama_perf_context_print:       total time =    1331.75 ms /    41 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     342.73 ms /    21 tokens (   16.32 ms per token,    61.27 tokens per second)

llama_perf_context_print:        eval time =    7581.24 ms /   121 runs   (   62.65 ms per token,    15.96 tokens per second)

llama_perf_context_print:       total time =    8055.48 ms /   142 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =    1308.87 ms /    90 tokens (   14.54 ms per token,    68.76 tokens per second)

llama_perf_context_print:        eval time =   20071.04 ms /   315 runs   (   63.72 ms per token,    15.69 tokens per second)

llama_perf_context_print:       total time =   21781.51 ms /   405 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     596.54 ms /    40 tokens (   14.91 ms per token,    67.05 tokens per second)

llama_perf_context_print:        eval time =   11391.33 ms /   182 runs   (   62.59 ms per token,    15.98 tokens per second)

llama_perf_context_print:       total time =   12194.65 ms /   222 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     752.06 ms /    51 tokens (   14.75 ms per token,    67.81 tokens per second)

llama_perf_context_print:        eval time =    9082.31 ms /   145 runs   (   62.64 ms per token,    15.97 tokens per second)

llama_perf_context_print:       total time =    9993.93 ms /   196 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 56, 'completion_tokens': 8, 'total_tokens': 64}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 68, 'completion_tokens': 14, 'total_tokens': 82}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 62, 'completion_tokens': 121, 'total_tokens': 183}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 131, 'completion_tokens': 315, 'total_tokens': 446}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 81, 'completion_tokens': 182, 'total_tokens': 263}}
llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     867.03 ms /    58 tokens (   14.95 ms per token,    66.90 tokens per second)

llama_perf_context_print:        eval time =    3810.21 ms /    61 runs   (   62.46 ms per token,    16.01 tokens per second)

llama_perf_context_print:       total time =    4739.66 ms /   119 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     467.54 ms /    29 tokens (   16.12 ms per token,    62.03 tokens per second)

llama_perf_context_print:        eval time =    4924.59 ms /    79 runs   (   62.34 ms per token,    16.04 tokens per second)

llama_perf_context_print:       total time =    5474.52 ms /   108 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     363.26 ms /    24 tokens (   15.14 ms per token,    66.07 tokens per second)

llama_perf_context_print:        eval time =   17556.37 ms /   278 runs   (   63.15 ms per token,    15.83 tokens per second)

llama_perf_context_print:       total time =   18258.26 ms /   302 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     751.11 ms /    51 tokens (   14.73 ms per token,    67.90 tokens per second)

llama_perf_context_print:        eval time =    3832.95 ms /    61 runs   (   62.84 ms per token,    15.91 tokens per second)

llama_perf_context_print:       total time =    4646.73 ms /   112 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 92, 'completion_tokens': 145, 'total_tokens': 237}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 99, 'completion_tokens': 61, 'total_tokens': 160}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 69, 'completion_tokens': 79, 'total_tokens': 148}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 64, 'completion_tokens': 278, 'total_tokens': 342}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 92, 'completion_tokens': 61, 'total_tokens': 153}}
llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     851.21 ms /    58 tokens (   14.68 ms per token,    68.14 tokens per second)

llama_perf_context_print:        eval time =   19507.88 ms /   308 runs   (   63.34 ms per token,    15.79 tokens per second)

llama_perf_context_print:       total time =   20743.08 ms /   366 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     951.71 ms /    63 tokens (   15.11 ms per token,    66.20 tokens per second)

llama_perf_context_print:        eval time =   14406.09 ms /   228 runs   (   63.18 ms per token,    15.83 tokens per second)

llama_perf_context_print:       total time =   15625.78 ms /   291 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =    1451.46 ms /   102 tokens (   14.23 ms per token,    70.27 tokens per second)

llama_perf_context_print:        eval time =   10799.18 ms /   170 runs   (   63.52 ms per token,    15.74 tokens per second)

llama_perf_context_print:       total time =   12440.94 ms /   272 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 102, 'completion_tokens': 308, 'total_tokens': 410}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 107, 'completion_tokens': 228, 'total_tokens': 335}}
llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     754.24 ms /    51 tokens (   14.79 ms per token,    67.62 tokens per second)

llama_perf_context_print:        eval time =   16011.82 ms /   254 runs   (   63.04 ms per token,    15.86 tokens per second)

llama_perf_context_print:       total time =   17068.93 ms /   305 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     785.48 ms /    54 tokens (   14.55 ms per token,    68.75 tokens per second)

llama_perf_context_print:        eval time =    9909.30 ms /   158 runs   (   62.72 ms per token,    15.94 tokens per second)

llama_perf_context_print:       total time =   10873.33 ms /   212 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     524.17 ms /    34 tokens (   15.42 ms per token,    64.86 tokens per second)

llama_perf_context_print:        eval time =    8050.37 ms /   129 runs   (   62.41 ms per token,    16.02 tokens per second)

llama_perf_context_print:       total time =    8721.87 ms /   163 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     683.77 ms /    45 tokens (   15.19 ms per token,    65.81 tokens per second)

llama_perf_context_print:        eval time =   19211.29 ms /   304 runs   (   63.20 ms per token,    15.82 tokens per second)

llama_perf_context_print:       total time =   20274.84 ms /   349 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 143, 'completion_tokens': 170, 'total_tokens': 313}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 91, 'completion_tokens': 254, 'total_tokens': 345}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 94, 'completion_tokens': 158, 'total_tokens': 252}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 75, 'completion_tokens': 129, 'total_tokens': 204}}
llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =    1094.56 ms /    75 tokens (   14.59 ms per token,    68.52 tokens per second)

llama_perf_context_print:        eval time =   26747.52 ms /   411 runs   (   65.08 ms per token,    15.37 tokens per second)

llama_perf_context_print:       total time =   28405.60 ms /   486 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     634.20 ms /    42 tokens (   15.10 ms per token,    66.22 tokens per second)

llama_perf_context_print:        eval time =    8557.22 ms /   136 runs   (   62.92 ms per token,    15.89 tokens per second)

llama_perf_context_print:       total time =    9339.90 ms /   178 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 86, 'completion_tokens': 304, 'total_tokens': 390}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 116, 'completion_tokens': 411, 'total_tokens': 527}}
llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     449.01 ms /    29 tokens (   15.48 ms per token,    64.59 tokens per second)

llama_perf_context_print:        eval time =   18364.25 ms /   290 runs   (   63.33 ms per token,    15.79 tokens per second)

llama_perf_context_print:       total time =   19167.72 ms /   319 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     661.77 ms /    43 tokens (   15.39 ms per token,    64.98 tokens per second)

llama_perf_context_print:        eval time =    7738.67 ms /   123 runs   (   62.92 ms per token,    15.89 tokens per second)

llama_perf_context_print:       total time =    8532.53 ms /   166 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     529.32 ms /    35 tokens (   15.12 ms per token,    66.12 tokens per second)

llama_perf_context_print:        eval time =    7111.26 ms /   114 runs   (   62.38 ms per token,    16.03 tokens per second)

llama_perf_context_print:       total time =    7761.82 ms /   149 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     447.01 ms /    28 tokens (   15.96 ms per token,    62.64 tokens per second)

llama_perf_context_print:        eval time =    8984.61 ms /   144 runs   (   62.39 ms per token,    16.03 tokens per second)

llama_perf_context_print:       total time =    9588.41 ms /   172 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     354.48 ms /    21 tokens (   16.88 ms per token,    59.24 tokens per second)

llama_perf_context_print:        eval time =    2779.98 ms /    45 runs   (   61.78 ms per token,    16.19 tokens per second)

llama_perf_context_print:       total time =    3180.37 ms /    66 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 83, 'completion_tokens': 136, 'total_tokens': 219}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 70, 'completion_tokens': 290, 'total_tokens': 360}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 84, 'completion_tokens': 123, 'total_tokens': 207}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 76, 'completion_tokens': 114, 'total_tokens': 190}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 69, 'completion_tokens': 144, 'total_tokens': 213}}
llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     428.08 ms /    26 tokens (   16.46 ms per token,    60.74 tokens per second)

llama_perf_context_print:        eval time =   24971.33 ms /   393 runs   (   63.54 ms per token,    15.74 tokens per second)

llama_perf_context_print:       total time =   25930.64 ms /   419 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     490.79 ms /    33 tokens (   14.87 ms per token,    67.24 tokens per second)

llama_perf_context_print:        eval time =    4757.37 ms /    76 runs   (   62.60 ms per token,    15.98 tokens per second)

llama_perf_context_print:       total time =    5326.95 ms /   109 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     299.76 ms /    20 tokens (   14.99 ms per token,    66.72 tokens per second)

llama_perf_context_print:        eval time =   11634.68 ms /   185 runs   (   62.89 ms per token,    15.90 tokens per second)

llama_perf_context_print:       total time =   12143.96 ms /   205 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     289.34 ms /    15 tokens (   19.29 ms per token,    51.84 tokens per second)

llama_perf_context_print:        eval time =    1673.68 ms /    27 runs   (   61.99 ms per token,    16.13 tokens per second)

llama_perf_context_print:       total time =    1990.46 ms /    42 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 63, 'completion_tokens': 45, 'total_tokens': 108}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 66, 'completion_tokens': 393, 'total_tokens': 459}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 73, 'completion_tokens': 76, 'total_tokens': 149}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 61, 'completion_tokens': 185, 'total_tokens': 246}}
llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     325.44 ms /    19 tokens (   17.13 ms per token,    58.38 tokens per second)

llama_perf_context_print:        eval time =   25746.27 ms /   405 runs   (   63.57 ms per token,    15.73 tokens per second)

llama_perf_context_print:       total time =   26621.12 ms /   424 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     281.32 ms /    14 tokens (   20.09 ms per token,    49.76 tokens per second)

llama_perf_context_print:        eval time =     603.55 ms /    10 runs   (   60.36 ms per token,    16.57 tokens per second)

llama_perf_context_print:       total time =     895.75 ms /    24 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     387.55 ms /    23 tokens (   16.85 ms per token,    59.35 tokens per second)

llama_perf_context_print:        eval time =    1180.93 ms /    19 runs   (   62.15 ms per token,    16.09 tokens per second)

llama_perf_context_print:       total time =    1588.38 ms /    42 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     291.38 ms /    18 tokens (   16.19 ms per token,    61.77 tokens per second)

llama_perf_context_print:        eval time =   29128.00 ms /   458 runs   (   63.60 ms per token,    15.72 tokens per second)

llama_perf_context_print:       total time =   30057.33 ms /   476 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 56, 'completion_tokens': 27, 'total_tokens': 83}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 60, 'completion_tokens': 405, 'total_tokens': 465}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 55, 'completion_tokens': 10, 'total_tokens': 65}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 64, 'completion_tokens': 19, 'total_tokens': 83}}
llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     527.72 ms /    35 tokens (   15.08 ms per token,    66.32 tokens per second)

llama_perf_context_print:        eval time =    5604.10 ms /    89 runs   (   62.97 ms per token,    15.88 tokens per second)

llama_perf_context_print:       total time =    6229.03 ms /   124 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =    1356.36 ms /    93 tokens (   14.58 ms per token,    68.57 tokens per second)

llama_perf_context_print:        eval time =    3135.24 ms /    50 runs   (   62.70 ms per token,    15.95 tokens per second)

llama_perf_context_print:       total time =    4542.94 ms /   143 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =    1606.21 ms /   111 tokens (   14.47 ms per token,    69.11 tokens per second)

llama_perf_context_print:        eval time =    5100.37 ms /    81 runs   (   62.97 ms per token,    15.88 tokens per second)

llama_perf_context_print:       total time =    6791.86 ms /   192 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 59, 'completion_tokens': 458, 'total_tokens': 517}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 76, 'completion_tokens': 89, 'total_tokens': 165}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 134, 'completion_tokens': 50, 'total_tokens': 184}}
llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =    1877.17 ms /   133 tokens (   14.11 ms per token,    70.85 tokens per second)

llama_perf_context_print:        eval time =    3990.52 ms /    63 runs   (   63.34 ms per token,    15.79 tokens per second)

llama_perf_context_print:       total time =    5933.06 ms /   196 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =    1650.08 ms /   117 tokens (   14.10 ms per token,    70.91 tokens per second)

llama_perf_context_print:        eval time =    3815.10 ms /    61 runs   (   62.54 ms per token,    15.99 tokens per second)

llama_perf_context_print:       total time =    5528.71 ms /   178 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =    1092.38 ms /    76 tokens (   14.37 ms per token,    69.57 tokens per second)

llama_perf_context_print:        eval time =     986.89 ms /    16 runs   (   61.68 ms per token,    16.21 tokens per second)

llama_perf_context_print:       total time =    2100.09 ms /    92 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =    1917.39 ms /   133 tokens (   14.42 ms per token,    69.37 tokens per second)

llama_perf_context_print:        eval time =    5251.11 ms /    84 runs   (   62.51 ms per token,    16.00 tokens per second)

llama_perf_context_print:       total time =    7256.35 ms /   217 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 152, 'completion_tokens': 81, 'total_tokens': 233}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 174, 'completion_tokens': 63, 'total_tokens': 237}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 158, 'completion_tokens': 61, 'total_tokens': 219}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 117, 'completion_tokens': 16, 'total_tokens': 133}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 174, 'completion_tokens': 84, 'total_tokens': 258}}
llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =    1389.69 ms /    98 tokens (   14.18 ms per token,    70.52 tokens per second)

llama_perf_context_print:        eval time =    5658.48 ms /    90 runs   (   62.87 ms per token,    15.91 tokens per second)

llama_perf_context_print:       total time =    7147.93 ms /   188 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     452.86 ms /    27 tokens (   16.77 ms per token,    59.62 tokens per second)

llama_perf_context_print:        eval time =    7194.40 ms /   115 runs   (   62.56 ms per token,    15.98 tokens per second)

llama_perf_context_print:       total time =    7770.79 ms /   142 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     978.81 ms /    67 tokens (   14.61 ms per token,    68.45 tokens per second)

llama_perf_context_print:        eval time =    2021.87 ms /    32 runs   (   63.18 ms per token,    15.83 tokens per second)

llama_perf_context_print:       total time =    3033.38 ms /    99 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     369.71 ms /    22 tokens (   16.81 ms per token,    59.51 tokens per second)

llama_perf_context_print:        eval time =    3408.78 ms /    55 runs   (   61.98 ms per token,    16.13 tokens per second)

llama_perf_context_print:       total time =    3838.81 ms /    77 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     666.39 ms /    43 tokens (   15.50 ms per token,    64.53 tokens per second)

llama_perf_context_print:        eval time =    2025.25 ms /    32 runs   (   63.29 ms per token,    15.80 tokens per second)

llama_perf_context_print:       total time =    2724.30 ms /    75 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     894.34 ms /    59 tokens (   15.16 ms per token,    65.97 tokens per second)

llama_perf_context_print:        eval time =    3181.74 ms /    51 runs   (   62.39 ms per token,    16.03 tokens per second)

llama_perf_context_print:       total time =    4128.47 ms /   110 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     772.27 ms /    53 tokens (   14.57 ms per token,    68.63 tokens per second)

llama_perf_context_print:        eval time =    1924.23 ms /    31 runs   (   62.07 ms per token,    16.11 tokens per second)

llama_perf_context_print:       total time =    2728.09 ms /    84 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 139, 'completion_tokens': 90, 'total_tokens': 229}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 68, 'completion_tokens': 115, 'total_tokens': 183}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 108, 'completion_tokens': 32, 'total_tokens': 140}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 63, 'completion_tokens': 55, 'total_tokens': 118}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 84, 'completion_tokens': 32, 'total_tokens': 116}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 100, 'completion_tokens': 51, 'total_tokens': 151}}
llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =    1099.63 ms /    76 tokens (   14.47 ms per token,    69.11 tokens per second)

llama_perf_context_print:        eval time =    5473.16 ms /    87 runs   (   62.91 ms per token,    15.90 tokens per second)

llama_perf_context_print:       total time =    6664.57 ms /   163 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     952.27 ms /    66 tokens (   14.43 ms per token,    69.31 tokens per second)

llama_perf_context_print:        eval time =    2620.06 ms /    42 runs   (   62.38 ms per token,    16.03 tokens per second)

llama_perf_context_print:       total time =    3615.36 ms /   108 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     906.22 ms /    61 tokens (   14.86 ms per token,    67.31 tokens per second)

llama_perf_context_print:        eval time =    3077.74 ms /    49 runs   (   62.81 ms per token,    15.92 tokens per second)

llama_perf_context_print:       total time =    4033.99 ms /   110 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     744.56 ms /    52 tokens (   14.32 ms per token,    69.84 tokens per second)

llama_perf_context_print:        eval time =    2589.91 ms /    42 runs   (   61.66 ms per token,    16.22 tokens per second)

llama_perf_context_print:       total time =    3377.88 ms /    94 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     817.64 ms /    55 tokens (   14.87 ms per token,    67.27 tokens per second)

llama_perf_context_print:        eval time =    2490.45 ms /    40 runs   (   62.26 ms per token,    16.06 tokens per second)

llama_perf_context_print:       total time =    3349.00 ms /    95 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     672.61 ms /    45 tokens (   14.95 ms per token,    66.90 tokens per second)

llama_perf_context_print:        eval time =    1836.34 ms /    29 runs   (   63.32 ms per token,    15.79 tokens per second)

llama_perf_context_print:       total time =    2538.43 ms /    74 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     905.35 ms /    61 tokens (   14.84 ms per token,    67.38 tokens per second)

llama_perf_context_print:        eval time =    2481.91 ms /    40 runs   (   62.05 ms per token,    16.12 tokens per second)

llama_perf_context_print:       total time =    3427.91 ms /   101 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 94, 'completion_tokens': 31, 'total_tokens': 125}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 117, 'completion_tokens': 87, 'total_tokens': 204}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 107, 'completion_tokens': 42, 'total_tokens': 149}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 102, 'completion_tokens': 49, 'total_tokens': 151}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 93, 'completion_tokens': 42, 'total_tokens': 135}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 96, 'completion_tokens': 40, 'total_tokens': 136}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 86, 'completion_tokens': 29, 'total_tokens': 115}}
llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     288.13 ms /    15 tokens (   19.21 ms per token,    52.06 tokens per second)

llama_perf_context_print:        eval time =    2656.75 ms /    43 runs   (   61.78 ms per token,    16.19 tokens per second)

llama_perf_context_print:       total time =    2988.52 ms /    58 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     870.70 ms /    59 tokens (   14.76 ms per token,    67.76 tokens per second)

llama_perf_context_print:        eval time =    1249.10 ms /    20 runs   (   62.46 ms per token,    16.01 tokens per second)

llama_perf_context_print:       total time =    2140.35 ms /    79 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     264.92 ms /    17 tokens (   15.58 ms per token,    64.17 tokens per second)

llama_perf_context_print:        eval time =     455.46 ms /     7 runs   (   65.07 ms per token,    15.37 tokens per second)

llama_perf_context_print:       total time =     728.30 ms /    24 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     724.76 ms /    47 tokens (   15.42 ms per token,    64.85 tokens per second)

llama_perf_context_print:        eval time =    1434.51 ms /    23 runs   (   62.37 ms per token,    16.03 tokens per second)

llama_perf_context_print:       total time =    2182.94 ms /    70 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     328.84 ms /    21 tokens (   15.66 ms per token,    63.86 tokens per second)

llama_perf_context_print:        eval time =    1008.78 ms /    16 runs   (   63.05 ms per token,    15.86 tokens per second)

llama_perf_context_print:       total time =    1354.30 ms /    37 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     709.13 ms /    49 tokens (   14.47 ms per token,    69.10 tokens per second)

llama_perf_context_print:        eval time =    1113.17 ms /    18 runs   (   61.84 ms per token,    16.17 tokens per second)

llama_perf_context_print:       total time =    1840.85 ms /    67 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     268.48 ms /    17 tokens (   15.79 ms per token,    63.32 tokens per second)

llama_perf_context_print:        eval time =     742.71 ms /    12 runs   (   61.89 ms per token,    16.16 tokens per second)

llama_perf_context_print:       total time =    1023.86 ms /    29 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =    1043.22 ms /    71 tokens (   14.69 ms per token,    68.06 tokens per second)

llama_perf_context_print:        eval time =    1182.96 ms /    19 runs   (   62.26 ms per token,    16.06 tokens per second)

llama_perf_context_print:       total time =    2245.82 ms /    90 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     295.36 ms /    20 tokens (   14.77 ms per token,    67.71 tokens per second)

llama_perf_context_print:        eval time =    1162.86 ms /    18 runs   (   64.60 ms per token,    15.48 tokens per second)

llama_perf_context_print:       total time =    1476.84 ms /    38 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 102, 'completion_tokens': 40, 'total_tokens': 142}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 56, 'completion_tokens': 43, 'total_tokens': 99}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 100, 'completion_tokens': 20, 'total_tokens': 120}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 58, 'completion_tokens': 7, 'total_tokens': 65}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 88, 'completion_tokens': 23, 'total_tokens': 111}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 62, 'completion_tokens': 16, 'total_tokens': 78}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 90, 'completion_tokens': 18, 'total_tokens': 108}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 58, 'completion_tokens': 12, 'total_tokens': 70}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 112, 'completion_tokens': 19, 'total_tokens': 131}}
llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     891.29 ms /    61 tokens (   14.61 ms per token,    68.44 tokens per second)

llama_perf_context_print:        eval time =    1137.78 ms /    18 runs   (   63.21 ms per token,    15.82 tokens per second)

llama_perf_context_print:       total time =    2047.73 ms /    79 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     649.24 ms /    44 tokens (   14.76 ms per token,    67.77 tokens per second)

llama_perf_context_print:        eval time =    1629.54 ms /    26 runs   (   62.67 ms per token,    15.96 tokens per second)

llama_perf_context_print:       total time =    2305.41 ms /    70 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     546.18 ms /    37 tokens (   14.76 ms per token,    67.74 tokens per second)

llama_perf_context_print:        eval time =    2910.38 ms /    47 runs   (   61.92 ms per token,    16.15 tokens per second)

llama_perf_context_print:       total time =    3504.54 ms /    84 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     593.99 ms /    40 tokens (   14.85 ms per token,    67.34 tokens per second)

llama_perf_context_print:        eval time =    3078.59 ms /    49 runs   (   62.83 ms per token,    15.92 tokens per second)

llama_perf_context_print:       total time =    3726.71 ms /    89 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     672.40 ms /    43 tokens (   15.64 ms per token,    63.95 tokens per second)

llama_perf_context_print:        eval time =    1412.15 ms /    23 runs   (   61.40 ms per token,    16.29 tokens per second)

llama_perf_context_print:       total time =    2108.12 ms /    66 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     823.24 ms /    55 tokens (   14.97 ms per token,    66.81 tokens per second)

llama_perf_context_print:        eval time =    2621.24 ms /    42 runs   (   62.41 ms per token,    16.02 tokens per second)

llama_perf_context_print:       total time =    3487.28 ms /    97 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     647.39 ms /    43 tokens (   15.06 ms per token,    66.42 tokens per second)

llama_perf_context_print:        eval time =     748.32 ms /    12 runs   (   62.36 ms per token,    16.04 tokens per second)

llama_perf_context_print:       total time =    1408.49 ms /    55 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     681.28 ms /    43 tokens (   15.84 ms per token,    63.12 tokens per second)

llama_perf_context_print:        eval time =    3165.27 ms /    51 runs   (   62.06 ms per token,    16.11 tokens per second)

llama_perf_context_print:       total time =    3898.74 ms /    94 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 60, 'completion_tokens': 18, 'total_tokens': 78}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 101, 'completion_tokens': 18, 'total_tokens': 119}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 85, 'completion_tokens': 26, 'total_tokens': 111}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 78, 'completion_tokens': 47, 'total_tokens': 125}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 81, 'completion_tokens': 49, 'total_tokens': 130}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 84, 'completion_tokens': 23, 'total_tokens': 107}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 96, 'completion_tokens': 42, 'total_tokens': 138}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 84, 'completion_tokens': 12, 'total_tokens': 96}}
llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     680.28 ms /    45 tokens (   15.12 ms per token,    66.15 tokens per second)

llama_perf_context_print:        eval time =    1918.81 ms /    31 runs   (   61.90 ms per token,    16.16 tokens per second)

llama_perf_context_print:       total time =    2630.81 ms /    76 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     688.06 ms /    46 tokens (   14.96 ms per token,    66.85 tokens per second)

llama_perf_context_print:        eval time =    2862.77 ms /    46 runs   (   62.23 ms per token,    16.07 tokens per second)

llama_perf_context_print:       total time =    3597.88 ms /    92 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     550.15 ms /    37 tokens (   14.87 ms per token,    67.25 tokens per second)

llama_perf_context_print:        eval time =    5562.16 ms /    89 runs   (   62.50 ms per token,    16.00 tokens per second)

llama_perf_context_print:       total time =    6209.75 ms /   126 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     813.65 ms /    57 tokens (   14.27 ms per token,    70.05 tokens per second)

llama_perf_context_print:        eval time =   14772.90 ms /   235 runs   (   62.86 ms per token,    15.91 tokens per second)

llama_perf_context_print:       total time =   15863.39 ms /   292 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     659.67 ms /    44 tokens (   14.99 ms per token,    66.70 tokens per second)

llama_perf_context_print:        eval time =   13183.84 ms /   211 runs   (   62.48 ms per token,    16.00 tokens per second)

llama_perf_context_print:       total time =   14087.76 ms /   255 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 84, 'completion_tokens': 51, 'total_tokens': 135}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 86, 'completion_tokens': 31, 'total_tokens': 117}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 87, 'completion_tokens': 46, 'total_tokens': 133}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 78, 'completion_tokens': 89, 'total_tokens': 167}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 87, 'completion_tokens': 235, 'total_tokens': 322}}
llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     755.48 ms /    51 tokens (   14.81 ms per token,    67.51 tokens per second)

llama_perf_context_print:        eval time =   11772.90 ms /   188 runs   (   62.62 ms per token,    15.97 tokens per second)

llama_perf_context_print:       total time =   12742.75 ms /   239 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =    1097.27 ms /    76 tokens (   14.44 ms per token,    69.26 tokens per second)

llama_perf_context_print:        eval time =   23606.78 ms /   372 runs   (   63.46 ms per token,    15.76 tokens per second)

llama_perf_context_print:       total time =   25204.93 ms /   448 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 74, 'completion_tokens': 211, 'total_tokens': 285}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 81, 'completion_tokens': 188, 'total_tokens': 269}}
llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     812.67 ms /    56 tokens (   14.51 ms per token,    68.91 tokens per second)

llama_perf_context_print:        eval time =    8986.22 ms /   144 runs   (   62.40 ms per token,    16.02 tokens per second)

llama_perf_context_print:       total time =    9956.69 ms /   200 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     873.44 ms /    58 tokens (   15.06 ms per token,    66.40 tokens per second)

llama_perf_context_print:        eval time =   16776.22 ms /   266 runs   (   63.07 ms per token,    15.86 tokens per second)

llama_perf_context_print:       total time =   17975.62 ms /   324 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     808.65 ms /    56 tokens (   14.44 ms per token,    69.25 tokens per second)

llama_perf_context_print:        eval time =   29980.63 ms /   469 runs   (   63.92 ms per token,    15.64 tokens per second)

llama_perf_context_print:       total time =   31460.77 ms /   525 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 106, 'completion_tokens': 372, 'total_tokens': 478}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 86, 'completion_tokens': 144, 'total_tokens': 230}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 89, 'completion_tokens': 266, 'total_tokens': 355}}
llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     450.91 ms /    29 tokens (   15.55 ms per token,    64.31 tokens per second)

llama_perf_context_print:        eval time =     375.27 ms /     6 runs   (   62.54 ms per token,    15.99 tokens per second)

llama_perf_context_print:       total time =     833.15 ms /    35 tokens

llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =     802.58 ms /    55 tokens (   14.59 ms per token,    68.53 tokens per second)

llama_perf_context_print:        eval time =   20976.60 ms /   331 runs   (   63.37 ms per token,    15.78 tokens per second)

llama_perf_context_print:       total time =   22203.12 ms /   386 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 86, 'completion_tokens': 469, 'total_tokens': 555}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 59, 'completion_tokens': 6, 'total_tokens': 65}}
llama_perf_context_print:        load time =     696.65 ms

llama_perf_context_print: prompt eval time =    1532.61 ms /   107 tokens (   14.32 ms per token,    69.82 tokens per second)

llama_perf_context_print:        eval time =   14017.91 ms /   222 runs   (   63.14 ms per token,    15.84 tokens per second)

llama_perf_context_print:       total time =   15810.55 ms /   329 tokens

logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 85, 'completion_tokens': 331, 'total_tokens': 416}}
logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 137, 'completion_tokens': 222, 'total_tokens': 359}}
